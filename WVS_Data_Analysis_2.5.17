---
title: "Cleaning WVS Data"
author: "Leigh Allison"
date: "March 8, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

First we need to tell R where are the data files are stored. 
```{r Set Workspace}
setwd("C:/Users/laall/Desktop/GitHub/WVS_Analysis_Leigh_Allison")
```

#Importing Data
I have subsetted the data from the Wave 6 WVS based on the integrated code book. In total there are 348 variables/questions. A couple of questions have two variables associated with them which will be explained later. 
```{r Load Raw Data}
PFL_data      <- read.csv("PFL_Raw_Data.csv")
Enviro_data   <- read.csv("Enviro_Raw_Data.csv")
Work_data     <- read.csv("Work_Raw_Data.csv")
Fam_data      <- read.csv("Fam_Raw_Data.csv")
PS_data       <- read.csv("PS_Raw_Data.csv")
RM_data       <- read.csv("RM_Raw_Data.csv")
Nation_data   <- read.csv("Nation_Raw_Data.csv")
Security_data <- read.csv("Security_Raw_Data.csv")
Science_data  <- read.csv("Science_Raw_Data.csv")
SD_data       <- read.csv("SD_Raw_Data.csv")

# We need the names in order to merge it with a world map. In order for the merge to work, we had to change the names to match the world map shape file. 
Country_Names <-read.csv("Country_Code_Names.csv")
Country_Names <-Country_Names[,c("Country.Code", "Country.Title")]
```

Right now, each row in the data file represents an indvidual response. In order to show national trends, each response needs to be condensed (aggregated) into one national metric. There are several different ways you can aggregate the data and several considerations about the data. 

First, the metric used depends on the type of question - categorical or ordinal. For categorical questions, the national statistic can be a mode or median (if the data is also ordinal).  For questions with a likert scale (oridnal/numerical questions), a national average is possible; however, the likert scale is interpreted differently be each respondant, creating a national average or mean that is not as meaningful. The WVS does not include any traditional numerical responses. However, all responses can be counted and sorted by the response/answer chosen by each individual respondant.  Counts can be compared across different questions and nations because they are unit of measure is the same - an single response by a respondant. 

Second, there are slight variations to the WVS depending on the country. For example  Iquestion 125 asks about political organizations, specific to each country. Each nation will have different response categories. Futhermore some questions were simpily not asked or were contionial on a repsondants previous questions response (Questions V90-V94). These questions were removed from this analysis, so that the number of reponses were approximately equal.  

Third, the goal of this analysis is to create principle components representing cultural values; therefore, the aggregation method used must allow for nations to be compared within a questions but also for questions to be compared and combined to understand national trends. Calculating a mean or median for each nationa allows for comparisons of different nations responses within single questions; but it does not allow us to combine questions into cultural values. Counts of responses are compariable across questions and nations as the unit of measurement is the same; however, it greatly increases the number of variables in the analysis. For each question, each response would have to be represented by a single variable - creating thousands of variables to analyze with only 60 countries in the sample. One way to reduce the number of variables would be to only use the variables representing the most popular (mode or median) response for each question. HOwever, large amounts of data are lost in this process.  

Finally, it is also important to consider tha these variables are going to be used in a principle component analysis and should therefore be on similar scale. One way to do this is to make the a national precentage of individuals that answered each variable by taking the count of response and dividing by the total number of people who answered the question. 

In order to understand the best course for analysis, we will first break the questions into categorical and ordinal data sets. 
```{r Loading Categorical Data}
PFL_Categorical_Data <- read.csv("PFL_Categorical_subset.csv")
Enviro_Categorical_Data <- read.csv("Enviro_Categorical_subset.csv")
Work_Categorical_Data <- read.csv("Work_Categorical_subset.csv")
Fam_Categorical_Data <- read.csv("Fam_Categorical_subset.csv")
PS_Categorical_Data <- read.csv("PS_Categorical_subset.csv")
RM_Categorical_Data <- read.csv("RM_Categorical_subset.csv")
Nation_Categorical_Data <- read.csv("Nation_Categorical_subset.csv")
Security_Categorical_Data <- read.csv("Security_Categorical_subset.csv")
#Science doesn't have categorical questions
SD_Categorical_Data <- read.csv("SD_Categorical_subset.csv")

Categorical_DF_List <- list(PFL_Categorical_Data, 
                            Enviro_Categorical_Data, 
                            Work_Categorical_Data, 
                            Fam_Categorical_Data,
                            PS_Categorical_Data, 
                            RM_Categorical_Data, 
                            Nation_Categorical_Data, 
                            Security_Categorical_Data,
                            SD_Categorical_Data)
```

```{r Loading Ordinal Data}
PFL_Ordinal_Data <- read.csv("PFL_Ordinal_subset.csv")
#Environmental questions are all categorical
Work_Ordinal_Data <- read.csv("Work_Ordinal_subset.csv")
#Family questions are all categorical
PS_Ordinal_Data <- read.csv("PS_Ordinal_subset.csv")
RM_Ordinal_Data <- read.csv("RM_Ordinal_subset.csv")
#Nation questions are all categorical
#Security questions are all categorical
Science_Ordinal_Data <- read.csv("Science_Ordinal_subset.csv")
#Socio-Demographic(SD) questions are all categorical

Ordinal_DF_List <- list(PFL_Ordinal_Data,  
                        Work_Ordinal_Data, 
                        PS_Ordinal_Data, 
                        RM_Ordinal_Data, 
                        Science_Ordinal_Data)
```

#Age
```{r}
SD_FreeResponse <- read.csv("FR_subset.csv")
Age_Cat <- 0
SD_FreeResponse <- cbind(SD_FreeResponse, Age_Cat)


SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 <= 20] <- "Under20"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 20 & SD_FreeResponse$V242 <= 24] <- "20-24"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 24 & SD_FreeResponse$V242 <= 29] <- "25-29"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 29 & SD_FreeResponse$V242 <= 34] <- "30-34"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 34 & SD_FreeResponse$V242 <= 39] <- "35-39"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 39 & SD_FreeResponse$V242 <= 44] <- "40-44"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 44 & SD_FreeResponse$V242 <= 49] <- "45-49"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 49 & SD_FreeResponse$V242 <= 54] <- "50-54"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 54 & SD_FreeResponse$V242 <= 59] <- "55-59"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 59 & SD_FreeResponse$V242 <= 64] <- "60-64"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 64 & SD_FreeResponse$V242 <= 69] <- "65-69"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 69 & SD_FreeResponse$V242 <= 74] <- "70-74"
SD_FreeResponse$Age_Cat[SD_FreeResponse$V242 > 74 & SD_FreeResponse$V242 <= 80] <- "75-80"
```

#Questions for Cleaning
In each list of questions and response the country code and weight are included. This steps removes the duplicate country code and weight columns from the questions being combined into one matrix.
```{r Define Variable Type}
#install.packages("dplyr")
library("dplyr")

#need to remove duplicate columns (country names and weight columns)
Categorical_subset <- as.data.frame(list(Categorical_DF_List) %>%
    Reduce(function(dtf1,dtf2) left_join(dtf1,dtf2,by="V2"), .))
Categorical_subset <-  subset(Categorical_subset, select=-c(V2.1,V2.2,V2.3,V2.4,V2.5,V2.6,V2.7,V2.8,                                                            X.1,X.2,X.3,X.4,X.5,X.6,X.7,X.8,                                                            V258.1, V258.2,V258.3,V258.4, V258.5,V258.6,V258.7, V258.8))

Ordinal_subset  <- as.data.frame(list(Ordinal_DF_List) %>%
    Reduce(function(dtf1,dtf2) left_join(dtf1,dtf2,by="V2"), .))
Ordinal_subset <- subset(Ordinal_subset, select=-c(V2.1,V2.2,V2.3,V2.4,
                                                    X.1,X.2,X.3,X.4,
                                                    V258.1, V258.2,V258.3, V258.4))
```

#Cleaning Data
When downloaded from the WVS wesite, this data was coded with numbers to represent the response chosen or why a response is missing. For example, -5 means missing or inapporiate response, while -4 respresents not asked in that survey.  In this next section of code, we converted -5 (inapplicable/refused/missing), -4 (not asked in survey), and -3 (not applicable), to -6 so that they are not included in the calculation. These (-6) responses represent an administractive decision; whereas -2 is when the respondant is asked the question and chooses not to answer and -1 is when the respondant answers that they do not know.

While the differences in these administratively missing responses have the potential to show interesting trends related to why a questions was or was not answered; the objective of this research is to determine how the reponses can be combined to determine cultural values, not how the survey setup are affecting the questions. Therefore, -2 as "no response" and -1 is "Don't know" and were left in the survey responses.

```{r Recode Package, warning=FALSE, results="hide"}
# Install and load the car package which contains the code that recodes cells and returns an updated dataframe
#install.packages("car", repos = 'http://cran.us.r-project.org')
library(car)
```
I created a function which will convert the missing values to NA using the recode function. Then I use the apply function to apply the function to every column in the dataframe.

```{r Recode Responses}
Recodetoneg6<- function(QuestionColName){
  QuestionColName<- recode(QuestionColName, "c(-5, -4, -3)= -6")
}

Updated_Categorical_data <- as.data.frame(apply(Categorical_subset,2,Recodetoneg6))
Updated_Ordinal_data <- as.data.frame(apply(Ordinal_subset,2,Recodetoneg6))
```
We now have two data matrices - one with categorical questions and one with ordinal questions. The rows are individual responses and the columns are questions with the exception of the first two columns which contain the country code and weight. 

#Educational Subgroups
```{r}
#Influcence of Education on Responses of a randomly selected question
Random_Question <- sample(1:224, 1)
colnames(Updated_Categorical_data)[Random_Question]
Education_Influence <- lm (Updated_Categorical_data[,Random_Question] ~ as.factor(Updated_Categorical_data$V248))
anova(Education_Influence)

#Influence of Education and Country on responses
Education_Country_Influence <- lm (Updated_Categorical_data[,Random_Question] ~ as.factor(Updated_Categorical_data$V2) 
                                   + as.factor(Updated_Categorical_data$V248))
anova(Education_Country_Influence)

#Since we saw a significant relationship between education and response category lets divide the data by Education level for future analysis purposes
Categorical_Educataion_Subgroups <- split(Updated_Categorical_data, f = Updated_Categorical_data$V248)
Categorical_Educataion_NoFormal <- as.data.frame(Categorical_Educataion_Subgroups$`1`)
Categorical_Educataion_IncompletePrimary <- as.data.frame(Categorical_Educataion_Subgroups$`2`)
Categorical_Educataion_CompletePrimary <- as.data.frame(Categorical_Educataion_Subgroups$`3`)
Categorical_Educataion_IncompleteSecondary_Tech <- as.data.frame(Categorical_Educataion_Subgroups$`4`)
Categorical_Educataion_CompleteSecondary_Tech <- as.data.frame(Categorical_Educataion_Subgroups$`5`)
Categorical_Educataion_IncompleteSecondary_Univ <- as.data.frame(Categorical_Educataion_Subgroups$`6`)
Categorical_Educataion_CompleteSecondary_Univ <- as.data.frame(Categorical_Educataion_Subgroups$`7`)
Categorical_Educataion_PartialUniversity <- as.data.frame(Categorical_Educataion_Subgroups$`8`)
Categorical_Educataion_UniversityDegree <- as.data.frame(Categorical_Educataion_Subgroups$`9`)

```

#Calculations for Categorical Questions - Counting
Now the precentage of people who answered each category for each questions can be computed by nation. In this first function we are counting the number of weighted responses to each questions response. The function returns a dataframe with the number of responses for each category as a column and nations as rows. Note, the function must be used in combination with the aggregate function in order to have the rows to be nations. We show an example in question calculation before applying it to all of the WVS questions.
```{r Categorical Question Count Function}
Count_Calc <- function(aColumn){
  miss=0
  r0=0
  r1=0
  r2=0
  r3=0
  r4=0
  r5=0
  r6=0
  r7=0
  r8=0
  rneg1=0
  rneg2=0
  rneg6=0
  
  count =1
  
  for (value in aColumn){
    if(is.na(value)){
      miss = miss + 1
      next
    }
    if(value == 0){
      r0 = r0 + Updated_Categorical_data$V258[count]
      next
    }
    
    if(value == 1){
      r1 = r1 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 2){
      r2 = r2 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 3){
      r3 = r3 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 4){
      r4 = r4 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 5){
      r5 = r5 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 6){
      r6 = r6 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 7){
      r7= r7 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == 8){
      r8 = r8 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == -1){
      rneg1 = rneg1 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == -2){
      rneg2 = rneg2 + Updated_Categorical_data$V258[count]
      next
    }
    if(value == -6){
      rneg6 = rneg6 + Updated_Categorical_data$V258[count]
      next
    }
    count = count + 1
  }
#NOTE: the majority questions do not have 9 categories.
  #rneg1 represents I don't know
  #rneg2 is a true none response or missing value
  #rneg6 is not applicable or not asked - so it was an administrative decision to not keep the question

  sumbyresponse <-rbind(r0, r1, r2, r3, r4, r5, r6, r7, r8, rneg1, rneg2, rneg6)
return(sumbyresponse)
}
```

#Example Categorical Question Calculation 
Before we run the analysis on all the questions, let's look at the break down of question V60. There are only 4  categorical responses, so the 5th category should have zero responses. 
```{r V60 Categorical Example}
#The result shows the number of people (worldwide who answered category 1, 2, 3, 4)
V60_Global <- Count_Calc(Updated_Categorical_data$V60)

#The result calculates the number of people who answered category 1, 2, 3, 4 by country
V60_Country_Breakdown <- aggregate(Updated_Categorical_data$V60, 
          by=list(Updated_Categorical_data$V2), 
          Count_Calc, simplify=FALSE)

#Need to reformat the above output of the aggregate function - used loop below
Question_Counts_Example <- data.frame()
for( i in 1:60){
 Count <- t(as.data.frame(V60_Country_Breakdown$x[i])) 
 Question_Counts_Example  <- as.data.frame(rbind(Question_Counts_Example, Count))
}
row.names(Question_Counts_Example ) <- as.character(V60_Country_Breakdown$Group.1)

#To calculate th precentage of people who answered each category by country. We start by summing the rows and then divide each count by the sum.
country_sums <- as.data.frame(apply(Question_Counts_Example , 1, sum)) 
V60_Precent <-Question_Counts_Example [1,]/country_sums$`apply(Question_Counts_Example, 1, sum)`[1]
V60_Precent
#check is the precentages were addded correctly
sum(V60_Precent[1,])
```

#Categorical Precentage Calculation 
To apply this function to the all the questions, we must use a "for" loop. The Count_Calc function is designed to count the responses to an individual question. The "for" loops allows the function to be applied to each column. As with the example the Count_Calc function will create dataframe for each function counting the number of respondants who selected which response and then the precentages (ranging for 0 to 100) with the nation are calculated. A table containing the reponse choice precentages by nation will be created and saved for each nation. The next step is to combine all of those dataframes into one dataframe with all the variables as columns and nations as rows. 

```{r Categorical Precentage Calculation}
#Create list of questions to loop function over 
Cat_Question_Column_Names=names(Updated_Categorical_data)[c(4:244)] 
#Create an empty list to name the count dataframes
Cat_Question_Column_Names_Count <- c()  

#Create a new variables to hold the count calculations
for(i in 1:241){ 
  Question_Name <- paste(Cat_Question_Column_Names[i], "Count", sep = "_")
  Cat_Question_Column_Names_Count <- c(Cat_Question_Column_Names_Count, Question_Name)
  }

PS_Percent_Cat_data <-data.frame() 
count = 1

#Apply the Count_Calc function to all the categorical questions.
for(col in Cat_Question_Column_Names){ #starting with first question
    #sumbyresponse_all dataframe contains the the categories as columns 
    #and the weighted sums for the number of people who answered in that category. 
    sumbyresponse_all <- aggregate(Updated_Categorical_data[,col], 
          by=list(Updated_Categorical_data$V2), 
          Count_Calc, simplify=FALSE)
   
   #Reformat the sumbyresponse_all dataframe into countries as rows & columns as categories
   Question_Counts <-data.frame()
   for(i in 1:60){
      Count <- t(as.data.frame(sumbyresponse_all$x[i])) 
      Question_Counts <- as.data.frame(rbind(Question_Counts, Count))
   }
   row.names(Question_Counts) <- as.character(sumbyresponse_all$Group.1)

   #To store the count dataframe under question name
   assign(Cat_Question_Column_Names_Count[count], Question_Counts)
   
   #Add up rows to determine total number of people from each country who answered each question
   country_sums <- as.data.frame(apply(Question_Counts, 1, sum)) 
   precentages <-data.frame() #this dataframe will store the precentage of people who answered each category

   for(i in 1:60){
     percent <- round(Question_Counts[i,]/country_sums$`apply(Question_Counts, 1, sum)`[i]*100,3)
     precentages <- rbind(precentages, percent)
   }

    colnames(precentages) <- c("0", "1_Ref", "2", "3", "4", "5", "6", "7", "8", "Neg1", "Missing", "AdminNA")
    Categorical_Percentages<- precentages 
  
  #Need to store the precent dataframe under question name
   colnames(Categorical_Percentages) <- paste(Cat_Question_Column_Names[count], colnames(Categorical_Percentages), sep = "_")
   assign(Cat_Question_Column_Names[count], Categorical_Percentages)
   count = count+1
} #ending analysis of one question looping back to start another question
```

#Full Categorical Data
We now will combine all the dataframes into one matirx. This data frame contains a column for each categorical response of each question for all questions in the Wave 6 of the WVS. The columns are first named with the question number and then by the coded category. Not all the questions have equal number of categories which is why some questionshave columns with all zeros.
```{r List of Categorical Data}
Cat_Precentage_DFs_list<- list(V4=V4, V5=V5, V6=V6, V7=V7, V8=V8, V9=V9, V10=V10,
                               V11=V11, V12=V12, V13=V13, V14=V14, V15=V15, V16=V16,
                               V17=V17, V18=V18, V19=V19, V20=V20, V21=V21, V22=V22,
                               V25=V25, V26=V26, V27=V27, V28=V28, V29=V29, V30=V30,  
                               V31=V31, V32=V32, V33=V33, V34=V34, V35=V35, V37=V37,
                               V42=V42, V39=V39, V38=V38, V36=V36, V40=V40, V41=V41,
                               V43=V43, V44=V44, V24=V24, V70=V70, V71=V71, V72=V72,
                               V73=V73, V75=V75, V76=V76, V77=V77, V78=V78, V79=V79,
                               V74=V74, V165=V165, V166=V166, V167=V167, 
                               V168=V168, V169=V169,V81=V81, V82=V82, V83=V83,V45=V45,V46=V46,
                               V102=V102, V49=V49, V54=V54, V51=V51, V52=V52, V50=V50, V48=V48, V47=V47, V53=V53,
                               V60=V60, V61=V61, V62=V62, V63=V63, V64=V64, V65=V65,V66=V66, V67=V67, V68=V68, V69=V69, 
                               V80=V80, V84=V84, V85=V85, V86=V86, V87=V87, V88=V88, V89=V89, 
                               V108=V108, V109=V109, V110=V110, V111=V111, V112=V112, V113=V113, V114=V114, V115=V115, 
                               V116=V116, V117=V117, V118=V118, V119=V119,
                               V120=V120, V121=V121, V122=V122, V123=V123, V124=V124,V126=V126,
                               V127=V127, V128=V128, V129=V129, V130=V130, V142=V142,
                               V217=V217, V218=V218, V219=V219, V220=V220, V221=V221, 
                               V222=V222, V223=V223, V224= V224,V225=V225, V226=V226, V227=V227,
                               V143=V143, V145=V145, V146=V146, V147=V147, V148=V148, V149=V149, V150=V150, V151=V151, 
                               V153=V153, V154=V154, V155=V155, V156=V156,
                               V211=V211, V103=V103, V104=V104, v106=V106, V107=V107,
                               V212=V212, V213=V213, V214=V214, V216=V216, V243=V243, V244=V244, V245=V245, V246=V246,
                               V170=V170, V171=V171, V172=V172, V173=V173, V174=V174, V175=V175, V176=V176, V177=V177, 
                               V178=V178, V179=V179, v180=V180, V181=V181, V182=V182, v183=V183, V184=V184, V185=V185, 
                               V186=V186, V187=V187, V188=V188, V189=V189, V190=V190, V191=V191,
                               V74B=V74B, V90=V90, V91=V91, V92=V92, V93=V93, V94=V94, 
                               v160A=V160A, v160B=V160B, v160C=V160C, v160D=V160D, v160E=V160E, v160F=V160F,
                               v160G=V160G, v160H=V160H, v160I=V160I, v160J=V160J,
                               V217_ESMA=V217_ESMA, V218_ESMA=V218_ESMA, V224_ESMA=V224_ESMA,
                               V220_ESMA=V220_ESMA, V221_ESMA=V221_ESMA, 
                               V222_ESMA=V222_ESMA, V228A=V228A, V228B=V228B, V228C=V228C,V228D=V228D,
                               V228E=V228E, V228F=V228F, V228G=V228G, V228H=V228H, V228I=V228I, V228J=V228J, V228K=V228K,
                               V243_AU=V243_AU,  V244_AU=V244_AU)     

#Questions 144, 215 (political organizations), 241, 242 (age), 247 (language), 249 (age of complete school),254 (ethnicity), 256 were removed beause they were coded using political organizations, religion, age, languages, regions and ethnicities 
#219_ESMA was removed because it was not answered by any of the 60 countries.
#228 and 228_2 were removed because they are related to political parties in specific countries
country.codes <- sumbyresponse_all$Group.1
```

Contained within this dataframe are the variables representing the precentage of people from each nation who did not respond to a questions as well as those who were not asked the questions. We want to remove countries and questions that have large amounts of administrative missingness. No repsonse is considered a choice be the respondant and could introduce interesting patterns. Therefore, the true missingness or no response catgory is kept in the analysis. We still created a separate dataframe inorder to visualize the missingness of both kinds. 

#Missing Data
```{r Visualizing Categorical Missing - No Response Data, eval=FALSE}
#Making a dataframe of columns for the missing responses - 
Missing_DF <- c()
for(df in Cat_Precentage_DFs_list){
  Missing_DF_add <- df[,11] 
  Missing_DF <- cbind(Missing_DF, Missing_DF_add)
}
Cat_Question_Column_Names_Missing <- names(Cat_Precentage_DFs_list)
colnames(Missing_DF) <- Cat_Question_Column_Names_Missing
rownames(Missing_DF) <- country.codes 

#Created a weighted scatterplot to determine which questions or countries should be removed. 
library(corrplot)
#install.packages("RColorBrewer")
library(RColorBrewer)

#Missing_DF1 <- Missing_DF[1:60,1:72]
#pdf("Missing_DF1_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Missing_DF1, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Missingness 1 of 3", mar = c(2,2,2,2))
#dev.off()

#Missing_DF2 <- Missing_DF[1:60,73:144]
#pdf("Missing_DF2_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Missing_DF2, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Missingness 2 of 3", mar = c(2,2,2,2))
#dev.off()

#Missing_DF3 <- Missing_DF[1:60,144:202]
#pdf("Missing_DF3_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Missing_DF3, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Missingness 3 of 3", mar = c(2,2,2,2))
#dev.off()

#pdf("Missing_DF_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(Missing_DF, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Missingness", mar = c(2,2,2,2))
#dev.off()
```

#AdminNA Data
```{r Visualizing Categorical AdminNA Data}
#Making a dataframe of columns for the Administravitive missing (AdminNA or -6) responses - 
AdminNA_DF <- c()
for(df in Cat_Precentage_DFs_list){
  AdminNA_DF_add <- as.numeric(df[,12]) 
  AdminNA_DF <- cbind(AdminNA_DF, AdminNA_DF_add)
}

AdminNA_DF<- as.matrix(AdminNA_DF)
Cat_Question_Column_Names_AdminNA <- names(Cat_Precentage_DFs_list)
colnames(AdminNA_DF) <- Cat_Question_Column_Names_AdminNA
rownames(AdminNA_DF) <- country.codes

summary(apply(AdminNA_DF, 2, function(x) any(is.na(x))))

any(is.na(AdminNA_DF) | is.infinite(AdminNA_DF))


#AdminNA_DF1 <- AdminNA_DF[1:60,1:72]
#pdf("AdminNA_DF1_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(AdminNA_DF1, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "AdminNA - 1 of 3", mar = c(2,2,2,2))
#dev.off()

#AdminNA_DF2 <- AdminNA_DF[1:60,73:144]
#pdf("AdminNA_DF2_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(AdminNA_DF2, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "AdminNA - 2 of 3", mar = c(2,2,2,2))
#dev.off()

#AdminNA_DF3 <- AdminNA_DF[1:60,144:202]
#pdf("AdminNA_DF3_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(AdminNA_DF3, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "AdminNA - 3 of 3", mar = c(2,2,2,2))
#dev.off()

#pdf("AdminNA_DF_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(AdminNA_DF, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "AdminNA", mar = c(2,2,2,2))
#dev.off()
```

#Don't Know Data
```{r Visualizing Dont Know Data}
#Making a dataframe of columns for the missing responses - 
Dontknow_DF <- c()
for(df in Cat_Precentage_DFs_list){
  Dontknow_DF_add <- df[,10] 
  Dontknow_DF <- cbind(Dontknow_DF, Dontknow_DF_add)
}
Cat_Question_Column_Names_Dontknow <- names(Cat_Precentage_DFs_list)
colnames(Dontknow_DF) <- Cat_Question_Column_Names_Dontknow
rownames(Dontknow_DF) <- country.codes


#Dontknow_DF1 <- Dontknow_DF[1:60,1:72]
#pdf("Dontknow_DF1_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Dontknow_DF1, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", #"gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Dontknow 1 of 3", mar = c(2,2,2,2))
#dev.off()

#Dontknow_DF2 <- Dontknow_DF[1:60,73:144]
#pdf("Dontknow_DF2_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Dontknow_DF2, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Dontknow 2 of 3", mar = c(2,2,2,2))
#dev.off()

#Dontknow_DF3 <-Dontknow_DF[1:60,144:202]
#pdf("Dontknow_DF3_Plot.pdf", width = 28, height = 14, paper="USr")
#corrplot(Dontknow_DF3, is.corr = FALSE, method = "color", tl.cex=0.4, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Dontknow 3 of 3", mar = c(2,2,2,2))
#dev.off()

#pdf("Dontknow_DF_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(Dontknow_DF, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Dontknow", mar = c(2,2,2,2))
#dev.off()
```

```{r Final Categorical Data Precentages}
#make one DF with all the single question dataframes
#Questions 144, 247,254, 256 were removed beause they were coded using religion, languages, regions and ethnicities 
#219_ESMA was removed because it was not answered by any of the 60 countries.
#228 and 228_2 were removed because they are related to political parties in specific countries
#Removed sociodemographic questions "V57"  "V58"  "V229" "V230" "V234" "V235" "V236" "V237" "V238" "V240" "V248" "V250" "V253" "V254" "V255" "V256"

Cat_Precentage_DFs <- cbind(V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V25,V26,V27,V28,
                           V29,V30,V31,V32,V33,V34,V35,V37,V42,V39,V38,V36,V40,V41,V43,V44,V24,V70,V71,V72,V73,V75,
                           V76, V77,V78,V79,V74,V165,V166,V167,V168,V169,V81,V82,V83,V45,V46,
                           V102,V49,V54,V51,V52,V50,V48,V47,V53,
                           V60,V61,V62,V63,V64,V65,V66,V67,V68,V69,V80,V84,V85,V86,V87,V88,V89,V108,V109,V110,V111,V112,
                           V113,V114,V115,V116,V117,V118,V119,V120,V121,V122,V123,V124,V126,V127,V128,V129,V130,V142,
                           V217,V218,V219,V220,V221,V222,V223,V224,V225,V226,V227,
                           V143,V145,V146,V147,V148,V149,V150,V151,V153,V154,V155,V156,
                           V211,V103,V104,V106,V107,V212,V213,V214,V216,V243,V244,V245,V246,
                           V170,V171,V172,V173,V174,V175,V176,V177,V178,V179,V180,V181,V182,V183,V184,V185,V186,V187,V188,
                           V189,V190,V191,
                           V74B, V90, V91, V92, V93, V94, 
                           V160A,V160B,V160C,V160D,V160E,V160F,V160G,V160H,V160I,V160J,
                           V217_ESMA, V218_ESMA, V224_ESMA, V220_ESMA, V221_ESMA, 
                           V222_ESMA, V228A, V228B, V228C,V228D,V228E,V228F, V228G, V228H, V228I,  V228J,V228K, V243_AU,
                           V244_AU)

#Remove the columns that are just zeros. This step removes categories that were not used by questions. 
Cat_Final_Precentage_DFs<- Cat_Precentage_DFs[,-(which(colSums(Cat_Precentage_DFs) == 0))] 
write.csv(Cat_Final_Precentage_DFs, file = "Precentage_Categorical_Data.csv")

#check that rows (representation nations) add to 100% by looking at questions V4
apply(Cat_Final_Precentage_DFs[,c(1:7)], 1, sum)
```

#Calculations for Ordinal Questions
Not all questions in the WVS have categorical responses. 51 questions ask respondants to respond on a likert scale. We first calculate the precentage of people who answered 1-10 as individual variables. Then we group responses 1-2 (Very low), 3-4 (Low), 5-6 (Neutral), 7-8 (High), 9-10 (Very High) - to make five categorical responses in order to understand the directionality of the data by adding the precentages of people who answered in these categories. 

First we need to aggregate the data into weighted sums for each country. The Likert_Count_Calc function is very similar to the Count_Calc function (in fact the only difference is it hasa more categories ot count). It returns a dataframe with countries as the rows and response categories 1 to 10, -1 ("I don't know"), -2 (No answer), and -6 (AdminNA). Recall that when downloaded from the WVS wesite, this data was coded with numbers to represent the response chosen or why a response is missing. For example, -5 means missing or inapporiate response, while -4 respresents not asked in that survey.  Earlier in this process (line 121), we converted -5 (inapplicable/refused/missing), -4 (not asked in survey), and -3 (not applicable), to -6 so that they are not included in the calculation. These (-6) responses represent an administractive decision; whereas -2 is when the respondant is asked the question and chooses not to answer and -1 is when the respondant answers that they do not know.

Similar to the Count_Calc function for the categorical questions. This function needs to be used with the aggregate function to separate by country and in a loop to analyze multiple questions at a time. We will start with an example using a single question (V95), then we will use a loop to apply the function to all of the ordinal questions in our data set.

```{r Ordinal Questions Count Function}
Likert_Count_Calc<- function(aColumn){
 
  miss=0
  r1=0
  r2=0
  r3=0
  r4=0
  r5=0
  r6=0
  r7=0
  r8=0
  r9=0
  r10=0
  rneg1=0
  rneg2=0
  rneg6=0
  
  count = 1
  
for (value in aColumn){
    
    if(is.na(value)){
      miss = miss + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 1){
      r1 = r1 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 2){
      r2 = r2 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 3){
      r3 = r3 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 4){
      r4 = r4 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 5){
      r5 = r5 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 6){
      r6 = r6 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 7){
      r7 = r7 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 8){
      r8 = r8 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 9){
      r9 = r9 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == 10){
      r10 = r10 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == -1){
      rneg1 = rneg1 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == -2){
      rneg2 = rneg2 + Updated_Ordinal_data$V258[count]
      next
    }
    if(value == -6){
      rneg6 = rneg6 + Updated_Ordinal_data$V258[count]
      next
    }
    count = count + 1
  }
  #Note that NA responses not included in subset. 
  dataofresponses_subset<-rbind(r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, rneg1, rneg2, rneg6) 
  
  #each column is a response sum, each row is coutry
  return(dataofresponses_subset)
}
```

#Example Question Calculation 
Before we run the analysis on all the questions, let's look at the break down of question V95. It is a ordinal questions with a likert scale of 1 to 10. 
```{r V95 Example}
Ordinal_Question_Counts_Example <- data.frame()
#The result below shows the number of people (worldwide who answered category 1, 2,...,10) - not divided by country
V95_Global <- Likert_Count_Calc(Updated_Ordinal_data$V95)

#Using the aggregate function, the number of responses per country is calculated. 
V95_Country_Breakdown<- aggregate(Updated_Ordinal_data$V95, 
          by=list(Updated_Ordinal_data$V2), 
          Likert_Count_Calc, simplify=FALSE)

#The result of aggregate function has to be reformatted into a dataframe that we will use later.
for( i in 1:60){
 Count <- t(as.data.frame(V95_Country_Breakdown$x[i])) 
 Ordinal_Question_Counts_Example  <- as.data.frame(rbind(Ordinal_Question_Counts_Example, Count))
}
row.names(Ordinal_Question_Counts_Example) <- as.character(V95_Country_Breakdown$Group.1)
#To calculate a precentage, we divide the number of responses in each category by the total number of people who answered the question from that country. The first step is to calculate the country sums. 
country_sums <- as.data.frame(apply(Ordinal_Question_Counts_Example, 1, sum)) 
#then we calculate the precentages. These precentages are for each individual response option.
V95_Precent <- Ordinal_Question_Counts_Example[1,]/country_sums$`apply(Ordinal_Question_Counts_Example, 1, sum)`[1]
V95_Precent
#Check to make sure they add to 1.
sum(V95_Precent[1,])
```

#Ordinal Question Count Calcuations
To determine the precentage of people who answered each response on the likert scales, we have to apply the Liket_Count_Calc function using the aggregate function within a loop. The loop will create a dataframe for each question. THe dataframe will have the countries as rows and the questions responses as columns. In each cell are the precentage of people who answered each response.
```{r Ordinal Percentage Calculations}
Ordinal_Question_Column_Names = names(Updated_Ordinal_data)[c(4:54)]
Ordinal_Question_Column_Names_Count =c() #List to name the count dataframes
count = 1

for(i in 1:51){ 
  Question_Name <- paste(Ordinal_Question_Column_Names[i], "Count", sep = "_")
  Ordinal_Question_Column_Names_Count <- c(Ordinal_Question_Column_Names_Count, Question_Name)
  }

#Loop through every question to create a dataframe with with counts of responses
for(col in Ordinal_Question_Column_Names){
        dataofresponses_country_subset <- aggregate(Updated_Ordinal_data[,col], 
                                   by=list(Updated_Ordinal_data$V2), 
                                   Likert_Count_Calc, simplify=FALSE)
        
#Reformat the dataframe into countries as rows and columns as categories
   Ordinal_Question_Counts <-data.frame()
   for(i in 1:60){
      Count <- t(as.data.frame(dataofresponses_country_subset$x[i])) 
      Ordinal_Question_Counts <- rbind(Ordinal_Question_Counts, Count)
   }
   row.names(Ordinal_Question_Counts) <- as.character(dataofresponses_country_subset$Group.1)
   
   #Add up rows to determine total number of people from each country who answered each question
   Ordinal_country_sums<- as.data.frame(apply(Ordinal_Question_Counts, 1, sum)) 
   
   #To store the count dataframe under question name
   assign(Ordinal_Question_Column_Names_Count[count], Ordinal_Question_Counts)
   
   #To calculate the precentages
   precentages<-data.frame() #this dataframe will store the precentage of people who answered each category

   for(i in 1:60){
     percent <- round(Ordinal_Question_Counts[i,]/Ordinal_country_sums$`apply(Ordinal_Question_Counts, 1, sum)`[i]*100,3)
     precentages <- rbind(precentages, percent)
   }

   Ordinal_Percentages<- precentages 
  
  #Need to store the precent dataframe under question name
   colnames(Ordinal_Percentages) <- paste(Ordinal_Question_Column_Names[count], colnames(Ordinal_Percentages), sep = "_")
   assign(Ordinal_Question_Column_Names[count], Ordinal_Percentages)
   count = count + 1
}#ending for loop to start a new question 
```

Now we need to group the ordinal responses and combine them into a single columns with new categorical titles. Let's start with our example question, V95
```{r Grouping Ordinal Resonses to Categorical Example V95}
#Sum responses to make into categories
sumNA <- function(x){sum(x,na.rm=TRUE)}

VLow_V95 <-as.data.frame(apply(V95[,c(1:2)],1,sumNA))
Low_V95 <-as.data.frame(apply(V95[,c(3:4)],1,sumNA))
Med_V95 <-as.data.frame(apply(V95[,c(5:6)],1,sumNA))
High_V95 <-as.data.frame(apply(V95[,c(7:8)],1,sumNA))
VHigh_V95 <-as.data.frame(apply(V95[,c(9:10)],1,sumNA))

V95_Categorized <-cbind(VLow_V95, Low_V95, Med_V95 , High_V95, VHigh_V95, 
                        V95$V95_rneg1  ,V95$V95_rneg2, V95$V95_rneg6 )
colnames(V95_Categorized) <- c(" Very Low", "Low", "Med", "High", " Very High", "Don't Know", "Missing", "AdminNA")

head(V95_Categorized)
```

To add the sum columns for all the ordinal questions, we will use a loop that sums the corresponding columns to create a new category for each question. THe loop will create new dataframes that are titled with the questions number and then the word "category" to indicate the question has been grouped into categories.
```{r}
count = 1
Ordinal_df <- list(V23,V56_NZ,V56,V55,V157,V158,V159,V160,V164,V59,V95,V96,V97,V98,V99,
                   V100,V101,V131,V132,V133,V134,V135,V136,
                   V137,V138,V140,V141,V192,V193,V194,V197,V152,V198,V200,V209,V210,V199,V201,V202,V203,V203A,
                   V204,V205,V207,V207A,
                   V206,V208,V231,V232,V233,V239)

Ordinal_Question_Category =c() #List to name the category dataframes

for(i in 1:51){ 
  Question_Name <- paste(Ordinal_Question_Column_Names[i], "Category", sep = "_")
  Ordinal_Question_Category <- c(Ordinal_Question_Category, Question_Name)
}

for(df in Ordinal_df){
  VLow<-as.data.frame(apply(df[,c(1:2)],1,sumNA))
  Low<-as.data.frame(apply(df[,c(3:4)],1,sumNA))
  Med<-as.data.frame(apply(df[,c(5:6)],1,sumNA))
  High<-as.data.frame(apply(df[,c(7:8)],1,sumNA))
  VHigh<-as.data.frame(apply(df[,c(9:10)],1,sumNA))
  DontKnow <- df[,11]
  Missing <-df[,12]
  NA_Admin <- df[,13]

Categorized <-cbind(VLow, Low, Med, High, VHigh, DontKnow, Missing, NA_Admin)
colnames(Categorized) <- c("Very Low_Ref", "Low", "Med", "High", " Very High", "Dont Know", "Missing", "NA_Admin")
colnames(Categorized) <- paste(Ordinal_Question_Column_Names[count], colnames(Categorized), sep = "_")
assign(Ordinal_Question_Category[count], Categorized)
count=count+1
}

#print(Ordinal_Question_Category)
```

#5 Item Likert Scale
There are three (V161,V162, V163) questions which are on a 1-5 scale. Since they are already divided into 5 categories, these questions had to be condensed into three cateogies. The three categories are Low (1-2), Neutral (3), High (4-5). 
```{r V161}
PFL_5Ordinal_Data <- read.csv("PFL_5Ordinal_subset.csv")

#V161
V161_Counts <- data.frame()
V161_Country_Breakdown<- aggregate(PFL_5Ordinal_Data$V161, 
          by=list(Updated_Ordinal_data$V2), 
          Count_Calc, simplify=FALSE)

for( i in 1:60){
 Count <- t(as.data.frame(V161_Country_Breakdown$x[i])) 
 V161_Counts<- as.data.frame(rbind(V161_Counts, Count))
}
row.names(V161_Counts) <- as.character(V161_Country_Breakdown$Group.1)
country_sums <- as.data.frame(apply(V161_Counts, 1, sum)) 

V161_Precent <- data.frame()
for(i in 1:60){
  Precent <- V161_Counts[i,]/country_sums$`apply(V161_Counts, 1, sum)`[i]
  V161_Precent <- rbind(V161_Precent, Precent)
}
#Sum responses 1-2, 3, and 4-5
sumNA <- function(x){sum(x,na.rm=TRUE)}

#VLow_V161 <-V161_Precent[,1]*100
Low_V161 <- as.data.frame(apply(V161_Precent[,c(1:2)],1,sumNA)*100)
Med_V161 <-V161_Precent[,3]*100
High_V161 <-as.data.frame(apply(V161_Precent[,c(4:5)],1,sumNA)*100)
#VHigh_V161 <-V161_Precent[,5]*100
DontKnow <- V161_Precent[,10]*100
Missing <-V161_Precent[,11]*100
NA_Admin <- V161_Precent[,12]*100

V161_Categorized <-cbind(Low_V161, Med_V161, High_V161, DontKnow, Missing, NA_Admin)
colnames(V161_Categorized) <- c("V161_Low_Ref", "V161_Med", "V161_High", "V161_Dont Know", "V161_Missing", "V161_NA_Admin")
head(V161_Categorized)

#check
sum(V161_Categorized[1,])
```
```{r V162}
V162_Counts <- data.frame()
V162_Country_Breakdown<- aggregate(PFL_5Ordinal_Data$V162, 
          by=list(Updated_Ordinal_data$V2), 
          Count_Calc, simplify=FALSE)
for( i in 1:60){
 Count <- t(as.data.frame(V162_Country_Breakdown$x[i])) 
 V162_Counts<- as.data.frame(rbind(V162_Counts, Count))
}
row.names(V162_Counts) <- as.character(V162_Country_Breakdown$Group.1)
country_sums <- as.data.frame(apply(V162_Counts, 1, sum)) 

V162_Precent <- data.frame()
for(i in 1:60){
  Precent <- V162_Counts[i,]/country_sums$`apply(V162_Counts, 1, sum)`[i]
  V162_Precent <- rbind(V162_Precent, Precent)
}

#Sum responses 1-2, 3, and 4-5
sumNA <- function(x){sum(x,na.rm=TRUE)}

#VLow_V162 <-V162_Precent[,1]*100
Low_V162 <- as.data.frame(apply(V162_Precent[,c(1:2)],1,sumNA)*100)
Med_V162 <-V162_Precent[,3]*100
High_V162 <-as.data.frame(apply(V162_Precent[,c(4:5)],1,sumNA)*100)
#VHigh_V162 <-V162_Precent[,5]*100
DontKnow <- V162_Precent[,10]*100
Missing <-V162_Precent[,11]*100
NA_Admin <- V162_Precent[,12]*100

V162_Categorized <-cbind(Low_V162, Med_V162, High_V162, DontKnow, Missing, NA_Admin)
colnames(V162_Categorized) <- c("V162_Low_Ref", "V162_Med", "V162_High", "V162_Dont Know", "V162_Missing", "V162_NA_Admin")
head(V162_Categorized)

#check
sum(V162_Categorized[1,])
```
```{r V163}
V163_Counts <- data.frame()
V163_Country_Breakdown<- aggregate(PFL_5Ordinal_Data$V163, 
          by=list(Updated_Ordinal_data$V2), 
         Count_Calc, simplify=FALSE)
for( i in 1:60){
 Count <- t(as.data.frame(V163_Country_Breakdown$x[i])) 
 V163_Counts<- as.data.frame(rbind(V163_Counts, Count))
}
row.names(V163_Counts) <- as.character(V163_Country_Breakdown$Group.1)
country_sums <- as.data.frame(apply(V163_Counts, 1, sum)) 

V163_Precent <- data.frame()
for(i in 1:60){
  Precent <- V163_Counts[i,]/country_sums$`apply(V163_Counts, 1, sum)`[i]
  V163_Precent <- rbind(V163_Precent, Precent)
}

#Sum responses 1-2, 3, and 4-5
sumNA <- function(x){sum(x,na.rm=TRUE)}

#VLow_V163 <-V163_Precent[,1]*100
Low_V163 <- as.data.frame(apply(V163_Precent[,c(1:2)],1,sumNA)*100)
Med_V163 <-V163_Precent[,3]*100
High_V163 <-as.data.frame(apply(V163_Precent[,c(4:5)],1,sumNA)*100)
#VHigh_V163 <-V163_Precent[,5]*100
DontKnow <- V163_Precent[,10]*100
Missing <-V163_Precent[,11]*100
NA_Admin <- V163_Precent[,12]*100

V163_Categorized <-cbind(Low_V163, Med_V163, High_V163,DontKnow, Missing, NA_Admin)
colnames(V163_Categorized) <- c("V163_Low_Ref", "V163_Med", "V163_High", "V163_Dont Know", "V163_Missing", "V163_NA_Admin")
head(V163_Categorized)

#check
sum(V163_Categorized[1,])
```

```{r Combine 5Likert Scale Questions}
PFL_5Scale_Cat <- as.data.frame(cbind(V161_Categorized, V162_Categorized, V163_Categorized))
row.names(PFL_5Scale_Cat) <- row.names(V163_Counts)
```

#Combining Ordinal Dataframe
We need to combine the categorized ordinal variables into one data frame. We have to add the 5item Likert scale - later because it is a differnt size than the other dataframes.

```{r List of Ordinal Dataframes}
Ordinal_Precentage_DFs_list <-list(V23_Category,V56_NZ_Category,
                                   V56_Category,V55_Category,V157_Category,V158_Category,V159_Category,
                                   V160_Category,V164_Category,V59_Category,V95_Category,V96_Category,V97_Category,
                                   V98_Category,V99_Category,V100_Category,V101_Category,V131_Category,V132_Category,
                                   V133_Category,V134_Category,V135_Category,V136_Category,V137_Category,V138_Category,
                                   V140_Category,V141_Category,V192_Category,V193_Category,V194_Category,
                                   V197_Category,V152_Category,V198_Category,V200_Category,V209_Category,
                                   V210_Category,V199_Category,V201_Category,V202_Category,V203_Category, V203A_Category,
                                   V204_Category,V205_Category,V207_Category,V207A_Category, V206_Category,V208_Category,
                                   V231_Category,V232_Category,V233_Category,V239_Category)
```

Contained within this dataframe are the variables representing the precentage of people from each nation who did not respond to a questions as well as those who were not asked the questions. We want to remove countries and questions that have large amounts of administrative missingness. No repsonse is considered a choice be the respondant and could introduce interesting patterns. Therefore, the true missingness or no response catgory is kept in the analysis. We still created a separate dataframe inorder to visualize the missingness of both kinds. 

```{r Visualizing Ordinal Missing - No Response Data}
#Making a dataframe with just the missing variables
Missing_DF_Ordinal <- c()
for(df in Ordinal_Precentage_DFs_list){
  Missing_DF_add <- df[,7] 
  Missing_DF_Ordinal <- cbind(Missing_DF_Ordinal, Missing_DF_add)
}

Ordinal_Question_Column_Names_Missing <- names(Updated_Ordinal_data)[c(4:54)] #51 columns
rownames(Missing_DF_Ordinal) <- country.codes

#Add the Missing data from V161,162, and 163
Missing_DF_Ordinal <- cbind(Missing_DF_Ordinal, V161_Categorized[,5], V162_Categorized[,5], V163_Categorized[,5])
colnames(Missing_DF_Ordinal) <- c(Ordinal_Question_Column_Names_Missing, "V161", "V162", "V163")


#pdf("Missing_DF_Ordinal_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(Missing_DF_Ordinal, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "Missingness", mar = c(2,2,2,2))
#dev.off()
```

```{r Visualizing Ordinal AdminNA}
#Making a dataframe with just the missing variables
AdminNA_DF_Ordinal <- c()
for(df in Ordinal_Precentage_DFs_list){
  AdminNA_DF_add <- df[,8] 
  AdminNA_DF_Ordinal <- cbind(AdminNA_DF_Ordinal, AdminNA_DF_add)
}
rownames(AdminNA_DF_Ordinal) <- country.codes

#Add the Missing data from V161,162, and 163
AdminNA_DF_Ordinal <- cbind(AdminNA_DF_Ordinal, V161_Categorized[,6], V162_Categorized[,6], V163_Categorized[,6])
colnames(AdminNA_DF_Ordinal) <- c(Ordinal_Question_Column_Names_Missing, "V161", "V162", "V163")

#pdf("AdminNA_DF_Ordinal_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(AdminNA_DF_Ordinal, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "AdminNA", mar = c(2,2,2,2))
#dev.off()
```

```{r Visualizing Ordinal DontKnow }
DontKnow_DF_Ordinal <- c()
for(df in Ordinal_Precentage_DFs_list){
  DontKnow_DF_add <- df[,5] 
  DontKnow_DF_Ordinal <- cbind(DontKnow_DF_Ordinal, DontKnow_DF_add)
}
rownames(DontKnow_DF_Ordinal) <- country.codes

#Add the Missing data from V161,162, and 163
DontKnow_DF_Ordinal <- cbind(DontKnow_DF_Ordinal, V161_Categorized[,5], V162_Categorized[,5], V163_Categorized[,5])
colnames(DontKnow_DF_Ordinal) <- c(Ordinal_Question_Column_Names_Missing, "V161", "V162", "V163")

#pdf("DontKnow_DF_Ordinal_Plot.pdf", width = 3000, height = 500, paper="USr")
#corrplot(DontKnow_DF_Ordinal, is.corr = FALSE, method = "color", tl.cex=0.5, tl.col= "black" , col= colorRampPalette(c("blue", "gray", "red"))(100), cl.lim=c(0,100), addgrid.col = "white", title = "DontKnow", mar = c(2,2,2,2))
#dev.off()
```

#Final Ordinal Dataframe
```{r Final Ordinal Dataframe}
Ordinal_Final_Precentage_DFs <- cbind(V23_Category,V56_NZ_Category,
                                   V56_Category,V55_Category,V157_Category,V158_Category,V159_Category,
                                   V160_Category,V164_Category,V59_Category,V95_Category,V96_Category,V97_Category,
                                   V98_Category,V99_Category,V100_Category,V101_Category,V131_Category,V132_Category,
                                   V133_Category,V134_Category,V135_Category,V136_Category,V137_Category,V138_Category,
                                   V140_Category,V141_Category,V192_Category,V193_Category,V194_Category,
                                   V197_Category,V152_Category,V198_Category,V200_Category,V209_Category,
                                   V210_Category,V199_Category,V201_Category,V202_Category,V203_Category, V203A_Category,
                                   V204_Category,V205_Category,V207_Category,V207A_Category, V206_Category,V208_Category,
                                   V231_Category,V232_Category,V233_Category,V239_Category,
                                   PFL_5Scale_Cat)

write.csv(Ordinal_Final_Precentage_DFs, file = "Precentage_Ordinal_Data.csv")

#check that questions add to 100%
apply(Ordinal_Final_Precentage_DFs[,c(1:8)], 1, sum)
```

#Socio Demographic Free Response Data
These questions are NOT included in the data set. 

#Merging Categorical and Ordinal Data sets
Now that both the categorical and ordinal variables are on the same scale, we can merge them and continue to a principle component analysis. 
```{r}
All_Data <- merge(Cat_Final_Precentage_DFs, Ordinal_Final_Precentage_DFs, by.x=0, by.y=0)
#NOTE: this data is missing questions 241, 242, 249 - these are free response demographic questions
write.csv(All_Data, file = "WVS_Data_Percentages_11.25.csv")
```

#Sensitivity Analysis for AdminNA (-6)
There are certain countries with high values of AdminNA values in the data frame. We have the choice of either removing variables or countries. 
```{r Combine all AdminaNA Columns}
Admin_DF_All<- as.data.frame(cbind(AdminNA_DF, AdminNA_DF_Ordinal))
#colSums(is.na(Admin_DF_All)) #double checking that there are no NAs
```

##Country Analysis for AdminNA
We want to eliminate any countries which did not answer large amounts of questions. We can visualize this by counting the number of cells in each row which equal 100. It is important to remember that these counts represent questions which were not answered because we are only looking at the AdminNA variable for each question.
```{r Country Sensitiviy Analsis 100 percent}
AdminNAQuestionCount <- c()
for(i in 1:60){
 count = sum(Admin_DF_All[i,] == 100) 
 AdminNAQuestionCount <- rbind(AdminNAQuestionCount, count)
 }
rownames(AdminNAQuestionCount) <- rownames(Admin_DF_All)
sort(AdminNAQuestionCount, decreasing = TRUE)
barplot(t(AdminNAQuestionCount), xlab = "Countries", ylab = "Number of Questions Skipped")
abline(h=10)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```

Let's also look at how many countries have more than 75% of AdminNA of all the variables.
```{r Country Sensitiviy Analsis 75 percent}
AdminNAQuestionCount75<- c()
for(i in 1:60){
 count = sum(Admin_DF_All[i,] >= 75) 
 AdminNAQuestionCount75 <- rbind(AdminNAQuestionCount75, count)
 }
rownames(AdminNAQuestionCount75) <- rownames(Admin_DF_All)
sort(AdminNAQuestionCount75, decreasing = TRUE)

barplot(t(AdminNAQuestionCount75), xlab = "Countries", ylab = "Number of Questions Skipped")
abline(h=10)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
From these plots and given the fact that we only have 60 countries we want to limit the number of countries we drop. There is clearly one country which skipped more questions than the rest.  That is country 643 - Qatar. It also appears there are 10-15 questions that the majority of countries skipped. We will look at those in the next step.

##Questions Analysis for AdminNA
We also want to elimnate questions with large amounts of administrative missing. We can understand the amount missing by counting the number of cells in each column which are greater than a certain percentage. We will look at 50, 75, 90, and 100.
```{r Question Sensitivy Analysis - 50 percent AdminNA}
QuestionCount50 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Admin_DF_All[,i] >= 50) 
 QuestionCount50 <- rbind(QuestionCount50, count)
}
rownames(QuestionCount50)<-colnames(Admin_DF_All) #label with question names

barplot(t(QuestionCount50), xlab = "Question", ylab = "Number of Countries who Skipped", main = "50% or Greater AdminNA")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")

barplot(t(QuestionCount50[c(150:256),1]), xlab = "Question", ylab = "Number of Countries who Skipped",
        main = "50% or Greater AdminNA", names.arg = rownames(QuestionCount50[c(150:256),]))
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis - 75 percent AdminNA}
QuestionCount75 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Admin_DF_All[,i] >= 75) 
 QuestionCount75 <- rbind(QuestionCount75, count)
}
rownames(QuestionCount75)<-colnames(Admin_DF_All) #label with question names

barplot(t(QuestionCount75), xlab = "Question", ylab = "Number of Countries who Skipped", main = "75% or Greater AdminNA")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")

barplot(t(QuestionCount75[c(150:256),1]), xlab = "Question", ylab = "Number of Countries who Skipped",
        main = "75% or Greater AdminNA", names.arg = rownames(QuestionCount75[c(150:256),]))
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis - 90 percent AdminNA}
QuestionCount90 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Admin_DF_All[,i] >= 90) 
 QuestionCount90 <- rbind(QuestionCount90, count)
}
rownames(QuestionCount90)<-colnames(Admin_DF_All) #label with question names

barplot(t(QuestionCount90), xlab = "Question", ylab = "Number of Countries who Skipped", main = "90% or Greater AdminNA")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")

barplot(t(QuestionCount90[c(150:256),1]), xlab = "Question", ylab = "Number of Countries who Skipped",
        main = "90% or Greater AdminNA", names.arg = rownames(QuestionCount90[c(150:256),]))
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis - 100 percent AdminNA}
QuestionCount100 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Admin_DF_All[,i] == 100) 
 QuestionCount100 <- rbind(QuestionCount100, count)
}
rownames(QuestionCount100)<-colnames(Admin_DF_All) #label with question names

barplot(t(QuestionCount100), xlab = "Question", ylab = "Number of Countries who Skipped", main = "100% AdminNA")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")

barplot(t(QuestionCount100[c(150:256),1]), xlab = "Question", ylab = "Number of Countries who Skipped",
        main = "100% AdminNA", names.arg = rownames(QuestionCount100[c(150:256),]))
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```

##Question Analysis for AdminNA (-6) Comparison
```{r Comparison of Questions Sensitvity AdminNA}
Comparison_Questions_AdminNA <- cbind(QuestionCount50,QuestionCount75,QuestionCount90,QuestionCount100)
matplot(Comparison_Questions_AdminNA[c(1:256),c(1:4)], type = c("b"), pch=1, col=1:4, xlab = "Question", ylab = "# of Countries who Skipped")
legend("topleft", legend = c("50", "75", "90", "100"), col=1:4, pch=1)
abline(h=10)
abline(h=20, col="red")
abline(h=15, col="grey")
abline(h=30, col="blue")

pdf("Comparison_Sensitivity.pdf")
matplot(Comparison_Questions_AdminNA[c(150:256),c(1:4)], type = c("b"), pch=1, col=1:4, xlab = "Question", ylab = "# of Countries who Skipped")
legend("topleft", legend = c("50", "75", "90", "100"), col=1:4, pch=1)
abline(h=10)
abline(h=20, col="red")
abline(h=15, col="grey")
abline(h=30, col="blue")
dev.off()
```

From the comparison, it seems that there is only small amounts of variation between 50% missing to 100% missing; therefore, we will remove all questions which have more than 15 countries missing at 50% or greater. In order to do this. We must create a list with those variables. 

```{r Removing Questions AdminNA}
#Remove the rows that are greater than 15 in from the 50% analysis.
QuestionstoRemove <- row.names(as.data.frame(which(QuestionCount50[,1] >= 15)))
Reduced_Questions <- subset(t(QuestionCount50), select = -c(which(QuestionCount50[,1] >= 15)))

Reduced_Admin_DF_All<- subset(Admin_DF_All, select = - c( V74B, V90,V91,V92,V93,V94,
                                                      v160A,v160B,v160C,v160D,v160E,v160F,v160G,v160H,v160I,v160J,
                                                      V217_ESMA,V218_ESMA,V224_ESMA,V220_ESMA,V221_ESMA,V222_ESMA,
                                                      V228A,V228B,V228C,V228D,V228E,V228F,V228G,V228H,V228I,V228J,V228K,
                                                      V243_AU,V244_AU,V56_NZ,V203A,V207A))

QuestionstoRemove
```

##Country Analysis for AdminNA
We will now repeat the analysis of the countries which skipped questions to see if there are a group of countries that omitted more than the rest. 
```{r Removing Countries AdminNA}
ReducedAdminNAQuestionCount <- c()
for(i in 1:60){
 count = sum(Reduced_Admin_DF_All[i,] == 100) 
 ReducedAdminNAQuestionCount <- rbind(ReducedAdminNAQuestionCount, count)
 }
rownames(ReducedAdminNAQuestionCount) <- rownames(Reduced_Admin_DF_All)
sort(ReducedAdminNAQuestionCount, decreasing = TRUE)

pdf("ReducedAdminNAQuestionCount.pdf")
barplot(t(ReducedAdminNAQuestionCount), xlab = "Countries", ylab = "Number of Questions Skipped")
abline(h=5)
abline(h=10, col="red")
abline(h=15, col="grey")
abline(h=20, col="blue")
dev.off()

#To see which questions they skipped
Removed_Countries_AdminNA <- Reduced_Admin_DF_All[rownames(Reduced_Admin_DF_All) %in% c(414, 634,48,818), ]

pdf("Questionstheyskipped.pdf")
matplot(t(Removed_Countries_AdminNA), type = c("p"), pch=1, col=1:4, xlab = "Question", ylab = "% of People not asked Question")
legend("topleft", legend = row.names(Removed_Countries_AdminNA), col=1:4, pch=1)
dev.off()
```
There are four countries which have skipped more than 20 questions. We will remove them from the anaylsis in the next section of code. They are all middle eastern countries; however from the graph we can see that they did skip the same questions 
414 - Kuwait
634 - Qatar
48 - Bahrain
818 - Egypt

#Sensitivity Analysis for Missing (-2)
There are certain countries with high values of AdminNA values in the data frame. We have the choice of either removing variables or countries. 
```{r Combine all Missing Columns}
Missing_DF <- c()
for(df in Cat_Precentage_DFs_list){
  Missing_DF_add <- df[,11] 
  Missing_DF <- cbind(Missing_DF, Missing_DF_add)
}
Cat_Question_Column_Names_Missing <- names(Cat_Precentage_DFs_list)
colnames(Missing_DF) <- Cat_Question_Column_Names_Missing
rownames(Missing_DF) <- country.codes 

Missing_DF_All<- as.data.frame(cbind(Missing_DF, Missing_DF_Ordinal))
#colSums(is.na(Missing_DF_All)) #double checking that there are no NAs
```

##Question Analysis for Missing (-2)
Let's also look at the missing responses. We can understand the amount missing by counting the number of cells in each column which are greater than a certain percentage. We will look at 50, 75, 90, and 100% and then compare them all.
```{r Question Sensitivy Analysis 50 percent Missing}
QuestionCount50 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Missing_DF_All[,i] >= 50) 
 QuestionCount50 <- rbind(QuestionCount50, count)
}
rownames(QuestionCount50)<-colnames(Missing_DF_All) #label with question names

barplot(t(QuestionCount50), xlab = "Question", ylab = "Number of Countries who Skipped", main = "50% or Greater Missing")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 75 percent Missing}
QuestionCount75 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Missing_DF_All[,i] >= 75) 
 QuestionCount75 <- rbind(QuestionCount75, count)
}
rownames(QuestionCount75)<-colnames(Missing_DF_All) #label with question names

barplot(t(QuestionCount75), xlab = "Question", ylab = "Number of Countries who Skipped", main = "75% or Greater Missing")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 90 percent Missing}
QuestionCount90 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Missing_DF_All[,i] >= 90) 
 QuestionCount90 <- rbind(QuestionCount90, count)
}
rownames(QuestionCount90)<-colnames(Missing_DF_All) #label with question names

barplot(t(QuestionCount90), xlab = "Question", ylab = "Number of Countries who Skipped", main = "90% or Greater Missing")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 100 percent Missing}
QuestionCount100 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(Missing_DF_All[,i] == 100) 
 QuestionCount100 <- rbind(QuestionCount100, count)
}
rownames(QuestionCount100)<-colnames(Missing_DF_All) #label with question names

barplot(t(QuestionCount100), xlab = "Question", ylab = "Number of Countries who Skipped", main = "100% Missing")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```

###Question Analysis for Missing (-2) Comparison
```{r Comparison of Questions Sensitvity Missing}
Comparison_Questions_Missing <- cbind(QuestionCount50,QuestionCount75,QuestionCount90,QuestionCount100)
pdf("Comparison_Sensitivity - Missing(-2).pdf")
matplot(Comparison_Questions_Missing[c(1:256),c(1:4)], type = c("b"), pch=1, col=1:4, xlab = "Question", ylab = "# of Countries who Skipped")
legend("topleft", legend = c("50", "75", "90", "100"), col=1:4, pch=1)
abline(h=10)
abline(h=20, col="red")
abline(h=15, col="grey")
abline(h=30, col="blue")
dev.off()
```

```{r Removing Questions Missing}
#Remove the rows that are greater than 15 in from the 50% analysis.
QuestionstoRemove <- row.names(as.data.frame(which(QuestionCount50[,1] >= 15)))
Reduced_Questions <- subset(t(QuestionCount50), select = -c(which(QuestionCount50[,1] >= 15)))

QuestionstoRemove 
```

#Sensitivity Analysis for DontKnow (-2)
There are certain countries with high values of DontKnow values in the data frame. We have the choice of either removing variables or countries. 
```{r Combine all DontKnow Columns}
DontKnow_DF_All<- as.data.frame(cbind(Dontknow_DF, DontKnow_DF_Ordinal))
#colSums(is.na(DontKnow_DF_All)) #double checking that there are no NAs
```

##Question Analysis for DontKnow (-1)
Let's also look at the Dont Know responses. We can understand the amount missing by counting the number of cells in each column which are greater than a certain percentage. We will look at 50, 75, 90, and 100% and then compare them all.
```{r Question Sensitivy Analysis 50 percent DontKnow}
QuestionCount50 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(DontKnow_DF_All[,i] >= 50) 
 QuestionCount50 <- rbind(QuestionCount50, count)
}
rownames(QuestionCount50)<-colnames(DontKnow_DF_All) #label with question names

barplot(t(QuestionCount50), xlab = "Question", ylab = "Number of Countries who Skipped", main = "50% or Greater DontKnow")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 75 percent DontKnow}
QuestionCount75 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(DontKnow_DF_All[,i] >= 75) 
 QuestionCount75 <- rbind(QuestionCount75, count)
}
rownames(QuestionCount75)<-colnames(DontKnow_DF_All) #label with question names

barplot(t(QuestionCount75), xlab = "Question", ylab = "Number of Countries who Skipped", main = "75% or Greater DontKnow")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 90 percent DontKnow}
QuestionCount90 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(DontKnow_DF_All[,i] >= 90) 
 QuestionCount90 <- rbind(QuestionCount90, count)
}
rownames(QuestionCount90)<-colnames(DontKnow_DF_All) #label with question names

barplot(t(QuestionCount90), xlab = "Question", ylab = "Number of Countries who Skipped", main = "90% or Greater DontKnow")
abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```
```{r Question Sensitivy Analysis 100 percent DontKnow}
QuestionCount100 = c()
count=0 #reset count

for(i in 1:256){
 count = sum(DontKnow_DF_All[,i] == 100) 
 QuestionCount100 <- rbind(QuestionCount100, count)
}
rownames(QuestionCount100)<-colnames(DontKnow_DF_All) #label with question names

barplot(t(QuestionCount100), xlab = "Question", ylab = "Number of Countries", main = "100% DontKnow")

abline(h=15)
abline(h=20, col="red")
abline(h=25, col="grey")
abline(h=30, col="blue")
```

###Question Analysis for DonntKnow (-1) Comparison
```{r Comparison of Questions Sensitvity DontKnow}
Comparison_Questions_DontKnow <- cbind(QuestionCount50,QuestionCount75,QuestionCount90, QuestionCount100)

matplot(Comparison_Questions_DontKnow[c(1:256),c(1:4)], type = c("b"), pch=1, col=1:4, xlab = "Question", ylab = "# of Countries")
legend("topleft", legend = c("50", "75", "90", "100"), col=1:4, pch=1)
abline(h=10)
abline(h=20, col="red")
abline(h=15, col="grey")
abline(h=30, col="blue")


pdf("Comparison_Sensitivity - DontKnow(-1).pdf")
matplot(Comparison_Questions_DontKnow[c(200:256),c(1:4)], type = c("b"), pch=1, col=1:4, xlab = "Question", ylab = "# of Countries")
legend("topleft", legend = c("50", "75", "90", "100"), col=1:4, pch=1)
abline(h=10)
abline(h=20, col="red")
abline(h=15, col="grey")
abline(h=30, col="blue")
dev.off()
```
```{r Removing Questions DontKnow}
#Remove the rows that are greater than 15 in from the 50% analysis.
QuestionstoRemove <- row.names(as.data.frame(which(QuestionCount50[,1] >= 15)))
Reduced_Questions <- subset(t(QuestionCount50), select = -c(which(QuestionCount50[,1] >= 15)))

QuestionstoRemove 
```

```{r Updated Combined Dataframe After Sensitivity Analysis}
#This step removes questions from sensitivity analysis above
Cat_Precentage_DFs <- cbind(V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V25,V26,V27,V28,
                           V29,V30,V31,V32,V33,V34,V35,V37,V42,V39,V38,V36,V40,V41,V43,V44,V24,V70,V71,V72,V73,V75,
                           V76, V77,V78,V79,V74,V165,V166,V167,V168,V169,V81,V82,V83,V45,V46,
                           V102,V49,V54,V51,V52,V50,V48,V47,V53,
                           V60,V61,V62,V63,V64,V65,V66,V67,V68,V69,V80,V84,V85,V86,V87,V88,V89,V108,V109,V110,V111,V112,
                           V113,V114,V115,V116,V117,V118,V119,V120,V121,V122,V123,V124,V126,V127,V128,V129,V130,V142,
                           V217,V218,V219,V220,V221,V222,V223,V224,V225,V226,V227,
                           V143,V145,V146,V147,V148,V149,V150,V151,V153,V154,V155,V156,
                           V211,V103,V104,V106,V107,V212,V213,V214,V216,V243,V244,V245,V246,
                           V170,V171,V172,V173,V174,V175,V176,V177,V178,V179,V180,V181,V182,V183,V184,V185,V186,V187,V188,
                           V189,V190,V191)

Cat_Question_Count <- list(V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V25,V26,V27,V28,
                           V29,V30,V31,V32,V33,V34,V35,V37,V42,V39,V38,V36,V40,V41,V43,V44,V24,V70,V71,V72,V73,V75,
                           V76, V77,V78,V79,V74,V165,V166,V167,V168,V169,V81,V82,V83,V45,V46,
                           V102,V49,V54,V51,V52,V50,V48,V47,V53,
                           V60,V61,V62,V63,V64,V65,V66,V67,V68,V69,V80,V84,V85,V86,V87,V88,V89,V108,V109,V110,V111,V112,
                           V113,V114,V115,V116,V117,V118,V119,V120,V121,V122,V123,V124,V126,V127,V128,V129,V130,V142,
                           V217,V218,V219,V220,V221,V222,V223,V224,V225,V226,V227,
                           V143,V145,V146,V147,V148,V149,V150,V151,V153,V154,V155,V156,
                           V211,V103,V104,V106,V107,V212,V213,V214,V216,V243,V244,V245,V246,
                           V170,V171,V172,V173,V174,V175,V176,V177,V178,V179,V180,V181,V182,V183,V184,V185,V186,V187,V188,
                           V189,V190,V191)

#Remove the columns that are just zeros. This step removes categories that were not used by questions. 
Cat_Final_Precentage_DFs<- Cat_Precentage_DFs[,-(which(colSums(Cat_Precentage_DFs) == 0))] 

Ordinal_Final_Precentage_DFs <- cbind(V23_Category,
                                   V56_Category,V55_Category,V157_Category,V158_Category,V159_Category,
                                   V160_Category,V164_Category,V59_Category,V95_Category,V96_Category,V97_Category,
                                   V98_Category,V99_Category,V100_Category,V101_Category,V131_Category,V132_Category,
                                   V134_Category,V135_Category,V136_Category,V137_Category,V138_Category,
                                   V141_Category,V192_Category,V194_Category,
                                   V197_Category,V198_Category,V200_Category,V209_Category,
                                   V210_Category,V199_Category,V201_Category,V202_Category,V203_Category,
                                   V204_Category,V205_Category,V207_Category, V206_Category,V208_Category,
                                   V231_Category,V232_Category,V233_Category,V239_Category,
                                   PFL_5Scale_Cat)

Oridnal_Question_Count <- list(V23_Category,
                                   V56_Category,V55_Category,V157_Category,V158_Category,V159_Category,
                                   V160_Category,V164_Category,V59_Category,V95_Category,V96_Category,V97_Category,
                                   V98_Category,V99_Category,V100_Category,V101_Category,V131_Category,V132_Category,
                                   V134_Category,V135_Category,V136_Category,V137_Category,V138_Category,
                                   V141_Category,V192_Category,V194_Category,
                                   V197_Category,V198_Category,V200_Category,V209_Category,
                                   V210_Category,V199_Category,V201_Category,V202_Category,V203_Category,
                                   V204_Category,V205_Category,V207_Category, V206_Category,V208_Category,
                                   V231_Category,V232_Category,V233_Category,V239_Category,
                                   PFL_5Scale_Cat)

```


#Merging Categorical and Ordinal Data sets
Now that both the categorical and ordinal variables are on the same scale, we can merge them and continue to a principle component analysis. 
```{r Categorical and Ordinal AFTER Sensitiviy Analysis}
Cleaned_Categorical_Ordinal_Data <- cbind(Cat_Final_Precentage_DFs,Ordinal_Final_Precentage_DFs)

#We need to remove the countries chosen above
Cleaned_Categorical_Ordinal_Data_56 <- Cleaned_Categorical_Ordinal_Data[!rownames( Cleaned_Categorical_Ordinal_Data) %in% c(414, 634,48,818), ]

#Now that we have completed the sensitiviy analysis we need to remove all the "AdminNA" Columns
WVS_Data_Precentages1<- Cleaned_Categorical_Ordinal_Data_56[, -grep("Admin", colnames(Cleaned_Categorical_Ordinal_Data_56))]

#Now that we have completed the sensitiviy analysis we need to remove all the "Missing" Columns
WVS_Data_Precentages2<- WVS_Data_Precentages1[, -grep("Missing", colnames(WVS_Data_Precentages1))]

#Now that we have completed the sensitiviy analysis we need to remove all the "Neg1" Columns
WVS_Data_Precentages3<- WVS_Data_Precentages2[, -grep("Neg1", colnames(WVS_Data_Precentages2))]
WVS_Data_Precentages4<- WVS_Data_Precentages3[, -grep("Know", colnames(WVS_Data_Precentages3))]

#write.csv(WVS_Data_Precentages, file = "WVS_Data_Percentages.csv")
```

#Analyzing the Characteristics of the Dummy Variables
## Analyzing the Characteristics of the Dummy Variables:Variance of Variables
Since the WVS has a variety of different question types, we wanted to look at the variance to determine if a specific type of question would mathematically appear in the components due to the structure of the question. Primarily we were concerned about questions which asked a yes or no question and those that asked a respondant to list 5 attributes they consider important and the  variables were coded as mentioned or not mentioned. We first looked at the variance of all the variables, then the two category responses (yes or no), then the lists (mention or not mentioned).
```{r All variables}
Variances <- c()
for(i in 1:835){
  Question_Variance <- var(WVS_Data_Precentages4[,i])
  Variances <- cbind(Variances, Question_Variance)
}
colnames(Variances)<-colnames(WVS_Data_Precentages4)
Variance_Sorted <- t(Variances)
#pdf("Variance distribution.pdf", paper = "USr")
par(mfrow=c(2, 1))
hist(Variances, 50)
abline(h=20, col = "red")
barplot(Variances)
#dev.off()
```

```{r Two Catergory Y/N Questions}
#Break into two groups - Questions with 2 answers and those with more than 2 answers. 
#Two answer questions: 12-22, 24, 36-44, 66, 82, 83, 148, 149, 150, 151, 176, 177, 178, 179, 180, 234, 235, 236, 240, 243, 244, 245,246, 250, 252

TwoCategoryQuestions<- WVS_Data_Precentages4[,c("V24_1_Ref","V24_2",
                                                "V66_1_Ref","V66_2",
                                                "V82_1_Ref","V82_2",
                                                "V83_1_Ref","V83_2" ,
                                                "V148_1_Ref","V148_2",
                                                "V149_1_Ref","V149_2",
                                                "V150_1_Ref","V150_2",
                                                "V151_1_Ref","V151_2",
                                                "V176_1_Ref","V176_5",
                                                "V177_1_Ref","V177_5",
                                                "V178_1_Ref","V178_5",
                                                "V179_1_Ref","V179_5",
                                                "V180_1_Ref","V180_5",
                                                "V187_1_Ref", "V187_2",
                                                "V243_1_Ref","V243_2",
                                                "V244_1_Ref","V244_2",
                                                "V245_1_Ref","V245_2",
                                                "V246_1_Ref","V246_2")]
Variances_TwoCategory<- c()
for(i in 1:36){
  Question_Variance_TwoCategory <- var(TwoCategoryQuestions[,i])
  Variances_TwoCategory <- cbind(Variances_TwoCategory, Question_Variance_TwoCategory)
}
colnames(Variances_TwoCategory)<-colnames(TwoCategoryQuestions)
Variance_Sorted_TwoCategory <- t(Variances_TwoCategory)
#pdf("Variance_TwoCategory distribution.pdf", paper = "USr")
par(mfrow=c(2, 1))
hist(Variances_TwoCategory)
barplot(Variances_TwoCategory)
#dev.off()
```

```{r Mention-Not Mention Questions}
MentionQuestions<- WVS_Data_Precentages4[,c("V12_1_Ref", "V12_2",
                                                       "V13_1_Ref", "V13_2",
                                                       "V14_1_Ref", "V14_2",
                                                       "V15_1_Ref", "V15_2",
                                                       "V16_1_Ref", "V16_2",
                                                       "V17_1_Ref", "V17_2",
                                                       "V18_1_Ref", "V18_2",
                                                       "V19_1_Ref", "V19_2",
                                                       "V20_1_Ref", "V20_2",
                                                       "V21_1_Ref", "V21_2",
                                                       "V22_1_Ref", "V22_2",
                                                       "V36_1_Ref", "V36_2",
                                                       "V37_1_Ref", "V37_2",
                                                       "V38_1_Ref", "V38_2",
                                                       "V39_1_Ref", "V39_2",
                                                       "V40_1_Ref", "V40_2",
                                                       "V41_1_Ref", "V41_2",
                                                       "V42_1_Ref", "V42_2",
                                                       "V43_1_Ref", "V43_2",
                                                       "V44_1_Ref", "V44_2")]
Variances_Mention<- c()
for(i in 1:40){
  Question_Variance_Mention <- var(MentionQuestions[,i])
  Variances_Mention <- cbind(Variances_Mention, Question_Variance_Mention)
}
colnames(Variances_Mention)<-colnames(MentionQuestions)
Variance_Sorted_Mention<- t(Variances_Mention)
#pdf("Variance_Mention distribution.pdf", paper = "USr")
par(mfrow=c(2, 1))
hist(Variances_Mention)
barplot(Variances_Mention)
#dev.off()
```

```{r Questions with 3 or more response categories}
remove <- c(colnames(TwoCategoryQuestions), colnames(MentionQuestions))
MultipleCat_Questions<- WVS_Data_Precentages4[,!colnames(WVS_Data_Precentages4) %in% remove]

Variances_MultipleCat<- c()
for(i in 1:759){
  Question_Variance_MultipleCat <- var(MultipleCat_Questions[,i])
  Variances_MultipleCat <- cbind(Variances_MultipleCat, Question_Variance_MultipleCat)
}
colnames(Variances_MultipleCat)<-colnames(MultipleCat_Questions)
Variance_Sorted_MultipleCat<- t(Variances_MultipleCat)

#pdf("Variance_MultipleCat distribution.pdf", paper = "USr")
par(mfrow=c(2, 1))
hist(Variances_MultipleCat)
barplot(Variances_MultipleCat)
#dev.off()
```

For these plots, we decided to remove the "no" and "not_mention" variables from the principle component analysis because the variance will load in equal but opposite directions and could mask other relationships. Since we are removing the first category of each questions we don't need to change the binary questions
```{r}
BinaryQuestions_RemoveVarible <- colnames(WVS_Data_Precentages4[,c("V12_2","V13_2", "V14_2", "V15_2", 
                                                       "V16_2", "V17_2", "V18_2", 
                                                       "V19_2",  "V20_2", "V21_2", 
                                                       "V22_2", "V36_2", "V37_2", 
                                                       "V38_2",  "V39_2", "V40_2", 
                                                       "V41_2",  "V42_2", "V43_2", 
                                                       "V44_2", "V24_2", "V66_2",
                                                       "V82_2","V83_2","V148_2","V149_2",
                                                       "V150_2", "V151_2","V176_5", "V177_5",
                                                       "V178_5","V179_5","V180_5","V187_2","V243_2",
                                                       "V244_2","V245_2", "V246_2")])
                                         
                                    
WVS_Data_Precentages5 <- WVS_Data_Precentages4[,!colnames(WVS_Data_Precentages4) %in% BinaryQuestions_RemoveVarible] 

#write.csv(WVS_Data_Precentages5, "MIRT_data.csv")
```

##Analyzing the Characteristics of the Dummy Variables: Normality Test
Multivariate Normality - requires variables to be independent
https://cran.r-project.org/web/packages/MVN/vignettes/MVN.pdf
```{r}
#install.packages("nortest")
library("nortest")

normality_test <- apply(WVS_Data_Precentages5, 2, ad.test)

#Need to determine which variables have significant p-values
normality_p.values <- c()
variable_names <- colnames(WVS_Data_Precentages5)

for(col in c(1:797)){
  variable_normality <- normality_test[[col]]
  p.value <- variable_normality$p.value
  normality_p.values <- cbind(normality_p.values, p.value)
}

colnames(normality_p.values) <-variable_names

#p-values less than 0.05 mean the variables are non-normal
nonnormal_variables <- t(normality_p.values)
nonnormal_variables[nonnormal_variables > 0.01] = NA
nonnormal_variables <- as.data.frame(t(na.omit(nonnormal_variables)))

normal_variables <- t(normality_p.values)
normal_variables[normal_variables < 0.01] = NA
normal_variables <- as.data.frame(t(na.omit(normal_variables)))
```
321 of the 797 varaiables are not normal according to the Anderson-Darling test of normality with a p-value of 0.0, leaving 476 normal variables

##Analyzing the Characteristics of the Dummy Variables: Intercorrelation between Variables
Let's take a look into how the variables are related - these relationships are a combination of lantent varaiables and the fact that they are dummy variables.
```{r}
#corrplot(cor(as.matrix(WVS_Data_Precentages5[,1:50])))
#corrplot(cor(as.matrix(WVS_Data_Precentages5[,51:100])))
#corrplot(cor(as.matrix(WVS_Data_Precentages5[,101:150])))
#corrplot(cor(V136_Count))
```
From this plot we can tell that the dummy variables are strongly correlated with other dummy variables from the same questions. This makes logical sense due to the way the dummy variables were created (i.e. porportion of the people from each country that answered that category). All the dummy variables for a single questions will sum to 100.

#Cluster Analysis
A simple way to see if variables are able to be dimensionally reduced is to perform a cluster analysis which analyses the points in space to determine which ones are "close" to each other.
```{r}
d <- dist(t(WVS_Data_Precentages5), method = "euclidean") # distance matrix
fit <- hclust(d, method="ward") 
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=5, border="red")
```
From this cluster analysis we should expect the varaibles to group into approximately 5 dimensions.

#Data for PCA
Using WVS 4 instead of five because we don't want to remove the binary questions completely.
```{r}
#Now that we have to remove our reference category before running the PCA. 
WVS_Data_Precentages6<- WVS_Data_Precentages4[, -grep("Ref", colnames(WVS_Data_Precentages4))]
#write.csv(WVS_Data_Precentages6, "PCA_Data_113017.csv")
```

#Prinicple Component Analysis
The goal of principle component analysis is to reduce a set of correlated variables to a smaller number of uncorrelated varaibles.The goal is create a new set of varaibles that accounts for as much variability as possible. These principle components are linear combinations of the previous variables.Therefore, each component has a loading from the variables it is made up of.  

When running a principle component analysis, either the correlation or covariance matrix can be used. the correlation matrix is primarily used when the data is on different scales. Since all the data is on a 0 to 100 scale, either correlation or covariance can be used. However, there must be more observations than variables to use the princomp function of R. Since our dataframe has more variables than observations, we sue the prcomp function which uses singular value decomposition to determine the components.

More details: 
http://www.sthda.com/english/wiki/principal-component-analysis-in-r-prcomp-vs-princomp-r-software-and-data-mining
 
```{r PCA Analysis}
#Based on observation of the varianances, we decided to remove half of the binary variables - since they are essentially equal and opposite. We kept on the positive responses. 

PCA_Analysis_Data <- WVS_Data_Precentages6
PCA_Analysis <-prcomp(PCA_Analysis_Data)
summary(PCA_Analysis)
```

```{r Scree Plot}
pdf("FinalScreePlot.pdf", width = 7, height = 5)
plot(PCA_Analysis, main = "ScreePlot", type = "l")
abline(h=5000, col="red")
dev.off()

plot(PCA_Analysis, main = "ScreePlot", type = "l")
abline(h=2500, col="red")
```

#PCA Result Analysis
```{r PCA Scores}
#Scores have countries as rows
PCA_Scores <- PCA_Analysis$x[,c(1:5)]
rownames(PCA_Scores) <- rownames(PCA_Analysis_Data)
head(PCA_Scores)

Named_PCA_Analysis_Data <- merge(Country_Names, PCA_Scores, by.y= 0, by.x ="Country.Code", all.y=T)
#Need to transform the data into a matrix for analysis later.
write.csv(Named_PCA_Analysis_Data, file="PCA_Scores_Named.csv")
```

```{r PCA Loadings}
#Loadings have variables as rows
PCA_Loadings <- round(PCA_Analysis$rotation [,c(1:5)], 3)
head(PCA_Loadings)
PCA_Loadings <- as.data.frame(PCA_Loadings)
write.csv(PCA_Loadings, file= "PCA_Loadings.csv")

#Plots of Components
#pdf("Loading distribution.pdf", paper = "USr")
par(mfrow=c(2,3))

plot(PCA_Loadings$PC1, type = "l", main = "PC1")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings$PC2, type = "l", main = "PC2")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings$PC3, type = "l", main = "PC3")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings$PC4, type = "l", main = "PC4")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings$PC5, type = "l", main = "PC5")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

#dev.off()
```

```{r Cleaning out values smaller than 0.1}
PCA_Loadings[abs(PCA_Loadings) < 0.1] = NA
PCA_Loadings_Cleaned <- PCA_Loadings[rowSums(is.na(PCA_Loadings))!= 5, ]
write.csv(PCA_Loadings_Cleaned, file= "PCA_Loadings_Cleaned_01.04.csv")
```

#Combining WVS with Renewable Energy Data  

##Renewable Generation Data 
The world bank collects the amount of renewable electricity is produced by a country in a given year. However, the available data is from a variety of years; however, I will only focus on production from 2013-2014. The code below creates a single list of countries with the most recent (either 2013 or 2014) renewable electricity generation. We then merge those valuse with the country codes and names. 
```{r Renewable_merge}
Renewable_Generation <- read.csv("RE_excludeHydro.csv")
Hydro_Generation <- read.csv("RE_includeHydro.csv")

#Not all of the countries have data for 2014, so I want to make a dataframe with a single most recent renewable generation as a percentage of their total production (excluding hydro)
Renewable_Generation_Most_Recent <- data.frame()
r <- 0

for(i in 1:263){
 if(is.na(Renewable_Generation$X2014[i])){
  #what to do if it is true
   (r <- Renewable_Generation$X2013[i])
}else {
  #what to do if it is false (i.e. 2014 is not NA)
  (r <- Renewable_Generation$X2014[i])
  }
 #print(r)
 Renewable_Generation_Most_Recent <- rbind(Renewable_Generation_Most_Recent, r)
}
#Now we make a named most recent renewable energy dataframe.
#Renewable_Generation_Most_Recent <- as.numeric(Renewable_Generation_Most_Recent)
Renewable_Generation_Most_Recent <-data.frame(Renewable_Generation_Most_Recent, Renewable_Generation[,1]) 
#do not make into a matrix otherwise will not merge with map below
colnames(Renewable_Generation_Most_Recent) <- c("RE_Generation", "Country.Name")

Renewable_Generation_Most_Recent <- merge(Renewable_Generation_Most_Recent, Hydro_Generation, 
                                          by.x = "Country.Name", by.y = "Country.Name")

Renewable_merge <-merge(Renewable_Generation_Most_Recent, Country_Names, 
                                   by.x="Country.Name", by.y="Country.Title")
Renewable_merge <- Renewable_merge[,c(-3)]
```

Let's visualize the renewable electiricty data in order to get a sense for it's distribution and scale.
We can see fromt the plot below that there are a considerable amount of very small values in the data set. In fact, in the histogram we can see that the data is skewwed to the left. A transformation of this data will be neccessary if we want to study linear relationships. 
```{r}
pdf("RE_Data.pdf", width = 7, height = 5)
plot(Renewable_merge$RE_Generation, xlim = c(0,60), ylim= c(0,30))
grid(30,30)
dev.off()

plot(Renewable_merge$Hyrdo2014, xlim = c(0,60), ylim= c(0,30))
grid(30,30)
```

```{r}
hist(Renewable_merge$Hyrdo2014, breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))
```

I decided to transform the data using a log transformation. I added one because of the multiple zeros in the data set and did want negative infinity in the resulting data set. In the transformed data set we can see that they data now ranges from 0 to 3. While the data is still not normal, there is considerably less skew. 
```{r}
transformed.data <- as.data.frame(log(Renewable_merge$Hyrdo2014+1))
rownames(transformed.data) <- Renewable_merge$Country.Code

#pdf("RE_Data_Transformed.pdf", width = 7, height = 5)
plot(log(Renewable_merge$Hyrdo2014+1), xlim = c(0,60), ylim = c(0,4))
grid(20,30)
hist(log(Renewable_merge$Hyrdo2014+1))
#dev.off()
```

#Merge RE and PS Data
Now that we have established princible components with country component scorse based on the majority of the WVS Survey questions in Wave 6, we can merge the renewable electricity data and the component scores by country. Unfortunatetly, 3 countries (Rwanda, Palestine, and Taiwan) are dropped from the analysis because they do not have renewable electricty data.
```{r}
Renewable_Scores <- merge(Renewable_merge, Named_PCA_Analysis_Data,
                                   by.x="Country.Name", by.y="Country.Title")
Renewable_Scores <- Renewable_Scores[,c(-4)]
Renewable_Scores <- Renewable_Scores[,c(1:7, 11:16)]
Renewable_Scores <- na.omit(Renewable_Scores)
#write.csv(Renewable_Scores, file = "Renewable_Scores.csv")
```

#Final Data for Regression
This is the dataframe we will use to explore the relationships between renewable electricity production and cultural values. 
```{r}
Final_Data <- Renewable_Scores
```

#Linear Regression
We will start by testing a linear regression with all 5 principle components and no control, moderator, or mediator variables
```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5)
summary(fullregression)
```
```{r}
fullregression <- lm(Final_Data$Hyrdo2014 ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5)
summary(fullregression)
```

#Controlling for GDP
There are many variabales associated with the contruction on new infrastructure. The availability of funds to build the infrastructure being a major factor. Therefore, in our regression models we will control for GDP. One country does not have a 2015 GDP values, bringing us down to 52 countries.
```{r GDP Control Data}
GDP_rawdata <- read.csv("World_Bank_GDP_Data.csv")
GDP_Years <- GDP_rawdata[4,]

#remove first 4 rows to bring column headers to top
GDP_data <- GDP_rawdata[c(5:286),]
colnames(GDP_data) <- GDP_Years

#remove all columns except for country names, and 2015 GDP
GDP_data2015 <-GDP_data[,c(1,60)]
colnames(GDP_data2015)<- c("Country.Title", "2015 GDP")

GDP_data_named<-merge(GDP_data2015, Country_Names, by.x= "Country.Title", by.y = "Country.Title")

GDP_Control <- merge(GDP_data_named, Final_Data, by.x = "Country.Title", by.y= "Country.Name")

Control <- GDP_Control$`2015 GDP`/10^9 #down to 53 countries
```

```{r}
fullregression <- lm(Final_Data$Hyrdo2014 ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5 + Control)
summary(fullregression)
```

#Other Controls
```{r}
Renewable_Resources <- read.csv("Renewable_Resources.csv")

Renewable_Resources<- merge(Renewable_Resources, Final_Data, by.x = "Country.Code", by.y = "Country.Code")

Renewable_Gen<- Renewable_Resources$Hyrdo2014
PC1 <- Renewable_Resources$PC1
PC2 <- Renewable_Resources$PC2
PC3 <- Renewable_Resources$PC3
PC4 <- Renewable_Resources$PC4
PC5 <- Renewable_Resources$PC5
Wind <- Renewable_Resources$Wind #this is based on region - needs to updated to country level
Solar <- as.numeric(Renewable_Resources$Solar) #this is based on region - needs to updated to country level
Policy <- Renewable_Resources$Policy.Target # this was created by divided the national renewable target by the number of years the country gave themselves to get there.
```

```{r}
fullregression_resources <- lm(Renewable_Gen ~ PC1 + PC2 + PC3 + PC4 + PC5 +
                                 Wind + Solar+Policy)
summary(fullregression_resources)
```
The regression relationship between renewable electricity generation (with controls of GDP, wind and solar resource, and policy support) and the prinicple components is significant. However, the only the first principle component, solar, and policy variables sinifianly contribute to this relationship.

#Linear Regression with Wind Cost Overrun Data
Now we are going to test the relationship of these principle components with cost overrun data collected by Sovacool et al. in 2017 (http://onlinelibrary.wiley.com/doi/10.1002/we.2069/abstract) 
```{r}
Wind_Cost_Overruns <- read.csv("Wind_Cost_Overrun_Data.csv")

Wind_Cost_Overruns_Final <- merge(Wind_Cost_Overruns, Country_Names, by.x = "Country", by.y = "Country.Title")
Cost_Performance_Wind <- merge(Wind_Cost_Overruns_Final, GDP_Control, by.x ="Country.Code", by.y = "Country.Code.x")
Cost_Performance_Wind_Resources <- merge(Cost_Performance_Wind, Renewable_Resources, 
                                         by.x ="Country.Code", by.y = "Country.Code")

fullregression <- lm(Cost_Performance_Wind_Resources$X.Cost.Over.Run.2017 ~ Cost_Performance_Wind_Resources$PC1.y +
                       Cost_Performance_Wind_Resources$PC2.y + Cost_Performance_Wind_Resources$PC3.y +
                       Cost_Performance_Wind_Resources$PC4.y + 
                       Cost_Performance_Wind_Resources$Offshore + 
                       Cost_Performance_Wind_Resources$Wind + Cost_Performance_Wind_Resources$Policy.Target)

summary(fullregression)
```
The regression relationship between wind project cost overrun (with controls of project type (onshore vs offshore) and wind resource) and the prinicple components is not significant. 

#Varaible Reduction 

## Varaible Reduction: Chi-Square Test
To reduce the amount of noise in the PCA analysis, we can remove questions that do not significantly vary with country. We use a chi-square signifance test on the contingency tables that we created when counting the number of responses for each dummy variable by country. A chi-square significance test determines if two categorical variables have a significant association with one another. For p-values less htan 0.05, we fail to reject the null hypothesis, implying that the variables are significantly  realted to each other. In this project, we are comparing the questions varaibles (separately) to the country variable. 

(More Information on Chi-squre:http://sites.stat.psu.edu/~ajw13/stat200_notes/12_assoc/10_assoc_print.htm)
```{r}
#Create a list of the dataframes that must run chi-square test on. 
#NOTE: Chi-square will not work on variables which contain zeros so those must be removed before running test.

Cat_Questions_CountDF<- list(V4_Count=V4_Count,V5_Count=V5_Count,V6_Count=V6_Count,V7_Count=V7_Count,
                             V8_Count=V8_Count,V9_Count=V9_Count,V10_Count=V10_Count,V11_Count=V11_Count,
                             V12_Count=V12_Count,V13_Count=V13_Count,V14_Count=V14_Count,V15_Count=V15_Count,
                             V16_Count=V16_Count,V17_Count=V17_Count,V18_Count=V18_Count,V19_Count=V19_Count,
                             V20_Count=V20_Count,V21_Count=V21_Count,V22_Count=V22_Count,V25_Count=V25_Count,
                             V26_Count=V26_Count,V27_Count=V27_Count,V28_Count=V28_Count,V29_Count=V29_Count,
                             V30_Count=V30_Count,V31_Count=V31_Count,V32_Count=V32_Count,V33_Count=V33_Count,
                             V34_Count=V34_Count,V35_Count=V35_Count,V37_Count=V37_Count,V42_Count=V42_Count,
                             V39_Count=V39_Count,V38_Count=V38_Count,V36_Count=V36_Count,V40_Count=V40_Count,
                             V41_Count=V41_Count,V43_Count=V43_Count,V44_Count=V44_Count,V24_Count=V24_Count,
                             V70_Count=V70_Count,V71_Count=V71_Count,V72_Count=V72_Count,V73_Count=V73_Count,
                             V75_Count=V75_Count,V76_Count=V76_Count,V77_Count=V77_Count,V78_Count=V78_Count,
                             V79_Count=V79_Count,V74_Count=V74_Count,V165_Count=V165_Count,V166_Count=V166_Count,
                             V167_Count=V167_Count,V168_Count=V168_Count,V169_Count=V169_Count,
                             V81_Count=V81_Count,V82_Count=V82_Count,V83_Count=V83_Count,V45_Count=V45_Count,
                             V46_Count=V46_Count,V102_Count=V102_Count,V49_Count=V49_Count,V54_Count=V54_Count,
                             V51_Count=V51_Count,V52_Count=V52_Count,V50_Count=V50_Count,V48_Count=V48_Count,
                             V47_Count=V47_Count,V53_Count=V53_Count,V60_Count=V60_Count,V61_Count=V61_Count,
                             V62_Count=V62_Count,V63_Count=V63_Count,V64_Count=V64_Count,V65_Count=V65_Count,
                             V66_Count=V66_Count,V67_Count=V67_Count,V68_Count=V68_Count,V69_Count=V69_Count,
                             V80_Count=V80_Count,V84_Count=V84_Count,V85_Count=V85_Count,V86_Count=V86_Count,
                             V87_Count=V87_Count,V88_Count=V88_Count,V89_Count=V89_Count,V108_Count=V108_Count,
                             V109_Count=V109_Count,V110_Count=V110_Count,V111_Count=V111_Count,V112_Count=V112_Count,
                             V113_Count=V113_Count,V114_Count=V114_Count,V115_Count=V115_Count,V116_Count=V116_Count,
                             V117_Count=V117_Count,V118_Count=V118_Count,V119_Count=V119_Count,V120_Count=V120_Count,
                             V121_Count=V121_Count,V122_Count=V122_Count,V123_Count=V123_Count,V124_Count=V124_Count,
                             V126_Count=V126_Count,V127_Count=V127_Count,V128_Count=V128_Count,V129_Count=V129_Count,
                             V130_Count=V130_Count,V142_Count=V142_Count,V217_Count=V217_Count,V218_Count=V218_Count,
                             V219_Count=V219_Count,V220_Count=V220_Count,V221_Count=V221_Count,V222_Count=V222_Count,
                             V223_Count=V223_Count,V224_Count=V224_Count,V225_Count=V225_Count,V226_Count=V226_Count,
                             V227_Count=V227_Count,V143_Count=V143_Count,V145_Count=V145_Count,V146_Count=V146_Count,
                             V147_Count=V147_Count,V148_Count=V148_Count,V149_Count=V149_Count,V150_Count=V150_Count,
                             V151_Count=V151_Count,V153_Count=V153_Count,V154_Count=V154_Count,V155_Count=V155_Count,
                             V156_Count=V156_Count,V211_Count=V211_Count,V103_Count=V103_Count,V104_Count=V104_Count,
                             V106_Count=V106_Count,V107_Count=V107_Count,V212_Count=V212_Count,V213_Count=V213_Count,
                             V214_Count=V214_Count,V216_Count=V216_Count,V243_Count=V243_Count,V244_Count=V244_Count,
                             V245_Count=V245_Count,V246_Count=V246_Count,V170_Count=V170_Count,V171_Count=V171_Count,
                             V172_Count=V172_Count,V173_Count=V173_Count,V174_Count=V174_Count,V175_Count=V175_Count,
                             V176_Count=V176_Count,V177_Count=V177_Count,V178_Count=V178_Count,V179_Count=V179_Count,
                             V180_Count=V180_Count,V181_Count=V181_Count,V182_Count=V182_Count,V183_Count=V183_Count,
                             V184_Count=V184_Count,V185_Count=V185_Count,V186_Count=V186_Count,V187_Count=V187_Count,
                             V188_Count=V188_Count,V189_Count=V189_Count,V190_Count=V190_Count,V191_Count=V191_Count)

Chisq_Signifance_Categorical <- c()
for (DF in Cat_Questions_CountDF){
  DF <- na.omit(DF)
  DF <- DF[,-(which(colSums(DF) == 0))] 
  Chisq_Signifance_Test <- chisq.test(DF, simulate.p.value = TRUE, B = 10000)
  Chisq_Signifance_Categorical <- rbind(Chisq_Signifance_Categorical, Chisq_Signifance_Test$p.value)
  }

#For the ordinal questions used the grouped varaibles, which are precentages - need to create new count tables that are grouped together by the categories - very low, low, medium, high, and very high

Ordinal_df_Count<- list(V23_Count,V56_Count,V55_Count,V157_Count,V158_Count,V159_Count,
                                   V160_Count,V164_Count,V59_Count,V95_Count,V96_Count,V97_Count,
                                   V98_Count,V99_Count,V100_Count,V101_Count,V131_Count,V132_Count,
                                   V134_Count,V135_Count,V136_Count,V137_Count,V138_Count,
                                   V141_Count,V192_Count,V194_Count,
                                   V197_Count,V198_Count,V200_Count,V209_Count,
                                   V210_Count,V199_Count,V201_Count,V202_Count,V203_Count,
                                   V204_Count,V205_Count,V207_Count,V206_Count,V208_Count,
                                   V231_Count,V232_Count,V233_Count,V239_Count)

Ordinal_Question_Column_Names<- c("V23","V56",'V55', 'V157',
                                'V158','V159','V159','V164',
                                'V59','V95','V96','V97',
                                'V98','V99','V100','V101',
                                'V131','V132','V134','V135',
                                'V136',"V137","V138", "V141",
                                'V192','V194','V197','V198','V200','V209',
                                'V210','V199','V201','V202','V203',
                                'V204','V205','V207', 'V206','V208',
                                'V231','V232','V233','V239')
count = 1
Ordinal_Question_Category_Count =c() #List to name the category dataframes

for(i in 1:44){ 
  Question_Name <- paste(Ordinal_Question_Column_Names[i], "Category_Count", sep = "_")
  Ordinal_Question_Category_Count <- c(Ordinal_Question_Category_Count, Question_Name)
}

for(df in Ordinal_df_Count){
  VLow<-as.data.frame(apply(df[,c(1:2)],1,sumNA))
  Low<-as.data.frame(apply(df[,c(3:4)],1,sumNA))
  Med<-as.data.frame(apply(df[,c(5:6)],1,sumNA))
  High<-as.data.frame(apply(df[,c(7:8)],1,sumNA))
  VHigh<-as.data.frame(apply(df[,c(9:10)],1,sumNA))
  DontKnow <- df[,11]
  Missing <-df[,12]
  NA_Admin <- df[,13]

Categorized <-cbind(VLow, Low, Med, High, VHigh, DontKnow, Missing, NA_Admin)
colnames(Categorized) <- c("Very Low_Ref", "Low", "Med", "High", " Very High", "Dont Know", "Missing", "NA_Admin")
colnames(Categorized) <- paste(Ordinal_Question_Column_Names[count], colnames(Categorized), sep = "_")
assign(Ordinal_Question_Category_Count[count], Categorized)
count=count+1
}

#VLow_V161 <-V161_Counts[,1]
Low_V161 <- as.data.frame(apply(V161_Counts[,c(1:2)],1,sumNA))
Med_V161 <-V161_Counts[,3]
High_V161 <-as.data.frame(apply(V161_Counts[,c(4:5)],1,sumNA))
#VHigh_V161 <-V161_Counts[,5]
DontKnow <- V161_Counts[,10]
Missing <-V161_Counts[,11]
NA_Admin <- V161_Counts[,12]

V161_Categorized_Count <-cbind(Low_V161, Med_V161, High_V161, DontKnow, Missing, NA_Admin)
colnames(V161_Categorized) <- c("V161_Low_Ref", "V161_Med", "V161_High", "V161_Dont Know", "V161_Missing", "V161_NA_Admin")

#VLow_V162 <-V162_Counts[,1]
Low_V162 <- as.data.frame(apply(V162_Counts[,c(1:2)],1,sumNA))
Med_V162 <-V162_Counts[,3]
High_V162 <-as.data.frame(apply(V162_Counts[,c(4:5)],1,sumNA))
#VHigh_V162 <-V162_Counts[,5]
DontKnow <- V162_Counts[,10]
Missing <-V162_Counts[,11]
NA_Admin <- V162_Counts[,12]

V162_Categorized_Count <-cbind(Low_V162, Med_V162, High_V162, DontKnow, Missing, NA_Admin)
colnames(V161_Categorized) <- c("V162_Low_Ref", "V162_Med", "V162_High", "V162_Dont Know", "V162_Missing", "V162_NA_Admin")

#VLow_V163 <-V163_Counts[,1]
Low_V163 <- as.data.frame(apply(V163_Counts[,c(1:2)],1,sumNA))
Med_V163 <-V163_Counts[,3]
High_V163 <-as.data.frame(apply(V163_Counts[,c(4:5)],1,sumNA))
#VHigh_V163 <-V163_Counts[,5]
DontKnow <- V163_Counts[,10]
Missing <-V163_Counts[,11]
NA_Admin <- V163_Counts[,12]

V163_Categorized_Count <-cbind(Low_V163, Med_V163, High_V163, DontKnow, Missing, NA_Admin)
colnames(V163_Categorized) <- c("V163_Low_Ref", "V163_Med", "V163_High", "V163_Dont Know", "V163_Missing", "V163_NA_Admin")

Ordinal_Question_CountDF<- list(V23_Category_Count=V23_Category_Count,V56_Category_Count=V56_Category_Count,
                                V55_Category_Count=V55_Category_Count, V157_Category_Count=V157_Category_Count,
                                V158_Category_Count=V158_Category_Count,V159_Category_Count=V159_Category_Count,
                                V159_Category_Count=V159_Category_Count,V164_Category_Count=V164_Category_Count,
                                V59_Category_Count=V59_Category_Count,V95_Category_Count=V95_Category_Count,
                                V96_Category_Count=V96_Category_Count,V97_Category_Count=V97_Category_Count,
                                V98_Category_Count=V98_Category_Count,V99_Category_Count=V99_Category_Count,
                                V100_Category_Count=V100_Category_Count,V101_Category_Count=V101_Category_Count,
                                V131_Category_Count=V131_Category_Count,V132_Category_Count=V132_Category_Count,
                                V134_Category_Count=V134_Category_Count,V135_Category_Count=V135_Category_Count,
                                V136_Category_Count=V136_Category_Count,V137_Category_Count=V137_Category_Count,
                                V138_Category_Count=V138_Category_Count,V141_Category_Count=V141_Category_Count,
                                V192_Category_Count=V192_Category_Count,V194_Category_Count=V194_Category_Count,
                                V197_Category_Count=V197_Category_Count,V198_Category_Count=V198_Category_Count,
                                V200_Category_Count=V200_Category_Count,V209_Category_Count=V209_Category_Count,
                                V210_Category_Count=V210_Category_Count,V199_Category_Count=V199_Category_Count,
                                V201_Category_Count=V201_Category_Count,V202_Category_Count=V202_Category_Count,
                                V203_Category_Count=V203_Category_Count,V204_Category_Count=V204_Category_Count,
                                V205_Category_Count=V205_Category_Count,V207_Category_Count=V207_Category_Count, 
                                V206_Category_Count=V206_Category_Count,V208_Category_Count=V208_Category_Count,
                                V231_Category_Count=V231_Category_Count,V232_Category_Count=V232_Category_Count,
                                V233_Category_Count=V233_Category_Count,V239_Category_Count=V239_Category_Count,
                                V161_Categorized_Count=V161_Categorized_Count[,c(1:5)],
                                V162_Categorized_Count=V162_Categorized_Count[,c(1:5)],
                                V163_Categorized_Count=V163_Categorized_Count[,c(1:5)])

Chisq_Signifance_Ordinal <- c()
for (DF in Ordinal_Question_CountDF){
  Chisq_Signifance_Test <- chisq.test(DF, simulate.p.value = TRUE, B = 10000)
  Chisq_Signifance_Ordinal <- rbind(Chisq_Signifance_Ordinal, Chisq_Signifance_Test$p.value)
}

#All the questions are signifinicantly impacted by the country according to the chi-sqare test of significance.
```

##Varaible Reduction: Discriminate Analysis
Since the chi-square test was not able to redce the noise, we could also run a discriminate analysis on all the dummy variables together. A discriminate analysis determines which numerical variabels is significantly contributing to a categorical response (dependent) varaible. To accomplish this we will use the WVS_Data_Precentages4 because it still contains all the reference variables. Unfortunantely since we only the one data point per country per variable, it is not possible to estimate using discriminate analysis. We also cannot use this on indvidual data because at that level the questions are nominal not numerical.

More information on choosing the right test (https://stats.idre.ucla.edu/other/mult-pkg/whatstat/)
```{r LDA analysis on WVS_Data_Precentages4, eval = FALSE}
#install.packages("MASS")
library("MASS")
WVS_Data_Precentages5_LDA<- add_rownames(WVS_Data_Precentages5, "Country")
lda(as.factor(Country) ~ ., data = WVS_Data_Precentages5_LDA)

WVS_Data_Precentages6_LDA<- add_rownames(WVS_Data_Precentages6, "Country")
lda(as.factor(Country) ~ ., data = WVS_Data_Precentages6_LDA)

```

##Varaible Reduction: PCA on Question's Precentage Dataframe
In order to reduce the number of variables going to into the PCA, we could reduce the dummy varaibles into dummy dimensions by conducting a PCA on the precentage dataframes. This will create one dimension for each questions representing all the dummy varaiables.
```{r}
PCA1_Scores <- matrix()

for (DF in Cat_Questions_CountDF){
  PCA_DF <- prcomp(DF)
  PCA1_Scores_DF <- as.data.frame(PCA_DF $x[,1])
  PCA1_Scores <- cbind(PCA1_Scores, PCA1_Scores_DF)
  }

for (DF in Ordinal_Question_CountDF){
  PCA_DF <- prcomp(DF)
  PCA1_Scores_DF <- as.data.frame(PCA_DF $x[,1])
  PCA1_Scores <- cbind(PCA1_Scores, PCA1_Scores_DF)
}

PCA1_Scores <- PCA1_Scores [,2:215]

PCA1_colnames <- c('V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16',
                   'V17','V18','V19','V20','V21',
                   'V22','V25','V26','V27','V28','V29','V30','V31','V32','V33','V34','V35',
                   'V37','V42','V39','V38','V36',
                   'V40','V41','V43','V44','V24','V70','V71','V72','V73','V75','V76','V77',
                   'V78','V79','V74','V165',
                   'V166','V167','V168','V169','V81','V82','V83','V45','V46',
                   'V102','V49','V54','V51','V52','V50',
                   'V48','V47','V53','V60','V61','V62','V63','V64','V65','V66','V67','V68',
                   'V69','V80','V84','V85',
                   'V86','V87','V88','V89','V108','V109','V110','V111','V112','V113','V114',
                   'V115','V116','V117',
                   'V118','V119','V120','V121','V122','V123','V124','V126','V127','V128','V129','V130','V142','V217',
                   'V218','V219','V220','V221','V222','V223','V224','V225','V226','V227','V143','V145','V146','V147',
                   'V148','V149','V150','V151','V153','V154','V155','V156','V211','V103','V104','V106','V107','V212',
                   'V213','V214','V216','V243','V244','V245','V246','V170','V171','V172','V173','V174','V175','V176',
                   'V177','V178','V179','V180','V181','V182','V183','V184','V185','V186','V187','V188','V189','V190',
                   'V191',
                   'V23_Category','V56_Category','V55_Category','V157_Category','V158_Category','V159_Category',
                   'V160_Category','V164_Category','V59_Category','V95_Category','V96_Category','V97_Category',
                   'V98_Category','V99_Category','V100_Category','V101_Category','V131_Category','V132_Category',
                   'V134_Category','V135_Category','V136_Category','V137_Category','V138_Category',
                   'V141_Category','V192_Category','V194_Category',
                   'V197_Category','V198_Category','V200_Category','V209_Category',
                   'V210_Category','V199_Category','V201_Category','V202_Category','V203_Category',
                   'V204_Category','V205_Category','V207_Category', 'V206_Category','V208_Category',
                   'V231_Category','V232_Category','V233_Category','V239_Category',
                   'V161_Categorized', 'V162_Categorized', 'V163_Categorized')

colnames(PCA1_Scores) <- PCA1_colnames

PCA_2 <- prcomp(PCA1_Scores)
summary(PCA_2)
plot(PCA_2, type = "l")

PCA_2.Varimax <-varimax(PCA_2$rotation)
head(PCA_2.Varimax$loadings)
```

```{r PCA2 Loadings}
#Loadings have variables as rows
PCA_Loadings2 <- round(PCA_2$rotation [,c(1:5)], 3)
head(PCA_Loadings2)
PCA_Loadings2 <- as.data.frame(PCA_Loadings2)
write.csv(PCA_Loadings, file= "PCA_Loadings2.csv")

#Plots of Components
#pdf("Loading distribution2.pdf", paper = "USr")
par(mfrow=c(2,3))

plot(PCA_Loadings2$PC1, type = "l", main = "PC1")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings2$PC2, type = "l", main = "PC2")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings2$PC3, type = "l", main = "PC3")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings2$PC4, type = "l", main = "PC4")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings2$PC5, type = "l", main = "PC5")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

#dev.off()
```
```{r Cleaning out values smaller than 0.1 - PCA2}
PCA_Loadings2[abs(PCA_Loadings2) < 0.1] = NA
PCA_Loadings_Cleaned2 <- PCA_Loadings2[rowSums(is.na(PCA_Loadings2))!= 5, ]
write.csv(PCA_Loadings_Cleaned2, file= "PCA_Loadings_Cleaned2.csv")
head(PCA_Loadings_Cleaned2)

PCA_Scores2 <- PCA_2$x[,c(1:5)]
head(PCA_Scores2)
#Need to manually remove countries from sensitivity analysis - 
#414 - Kuwait
#634 - Qatar
#48 - Bahrain
#818 - Egypt
Remove_Countries <- c(414,634,48,818)
PCA_Scores2 <- PCA_Scores2[ !(rownames(PCA_Scores2) %in% Remove_Countries), ]

Named_PCA_Analysis_Data2 <- merge(Country_Names, PCA_Scores2, by.y= 0, by.x ="Country.Code", all.y=T)

Renewable_Scores2 <- merge(Renewable_merge, Named_PCA_Analysis_Data2,
                                   by.x="Country.Name", by.y="Country.Title")

Renewable_Scores2 <- Renewable_Scores2[,c(1:7, 12:17)]
Renewable_Scores2 <- na.omit(Renewable_Scores2)
```

###Varaible Reduction: PCA on Question's Precentage Dataframe - Final Data for Regression
This is the dataframe we will use to explore the relationships between renewable electricity production and cultural values. 
```{r}
Final_Data <- Renewable_Scores2
```

###Varaible Reduction: PCA on Question's Precentage Dataframe - Linear Regression
We will start by testing a linear regression with all 5 principle components and no control, moderator, or mediator variables
```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5)
summary(fullregression)
```
###Varaible Reduction: PCA on Question's Precentage Dataframe - Control for GDP
There are many variabales associated with the contruction on new infrastructure. The availability of funds to build the infrastructure being a major factor. Therefore, in our regression models we will control for GDP. One country does not have a 2015 GDP values, bringing us down to 52 countries.
```{r GDP Control Data3}
GDP_rawdata <- read.csv("World_Bank_GDP_Data.csv")
GDP_Years <- GDP_rawdata[4,]

#remove first 4 rows to bring column headers to top
GDP_data <- GDP_rawdata[c(5:286),]
colnames(GDP_data) <- GDP_Years

#remove all columns except for country names, and 2015 GDP
GDP_data2015 <-GDP_data[,c(1,60)]
colnames(GDP_data2015)<- c("Country.Title", "2015 GDP")

GDP_data_named<-merge(GDP_data2015, Country_Names, by.x= "Country.Title", by.y = "Country.Title")

GDP_Control <- merge(GDP_data_named, Final_Data, by.x = "Country.Title", by.y= "Country.Name")

Control <- GDP_Control$`2015 GDP`/10^9 #down to 53 countries
```

```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5 + Control)
summary(fullregression)
```

###Varaible Reduction: PCA on Question's Precentage Dataframe - Other Controls
```{r}
Renewable_Resources <- read.csv("Renewable_Resources.csv")

Renewable_Resources<- merge(Renewable_Resources, Final_Data, by.x = "Country.Code", by.y = "Country.Code.x")

Renewable_Gen<- Renewable_Resources$RE_Generation.x
PC1 <- Renewable_Resources$PC1
PC2 <- Renewable_Resources$PC2
Wind <- Renewable_Resources$Wind
Solar <- as.numeric(Renewable_Resources$Solar)
Policy <- Renewable_Resources$Policy.Target
```

```{r}
fullregression_resources <- lm(log(Renewable_Gen+1) ~ PC1 + PC2 + PC3 + PC4 + PC5 +
                                 Wind + Solar + Policy)
summary(fullregression_resources)
```

###Varaible Reduction: PCA on Question's Precentage Dataframe - Linear Regression with Wind Cost Overrun Data
```{r}
Wind_Cost_Overruns <- read.csv("Wind_Cost_Overrun_Data.csv")

Wind_Cost_Overruns_Final <- merge(Wind_Cost_Overruns, Country_Names, by.x = "Country", by.y = "Country.Title")
Cost_Performance_Wind <- merge(Wind_Cost_Overruns_Final, GDP_Control, by.x ="Country.Code", by.y = "Country.Code")
Cost_Performance_Wind_Resources <- merge(Cost_Performance_Wind, Renewable_Resources, 
                                         by.x ="Country.Code", by.y = "Country.Code")

fullregression <- lm(Cost_Performance_Wind$X.Cost.Over.Run.2017 ~ Cost_Performance_Wind$PC1 + 
                       Cost_Performance_Wind$PC2 + 
                       Cost_Performance_Wind$PC3 + 
                       Cost_Performance_Wind$PC4 + 
                       Cost_Performance_Wind$Offshore + Cost_Performance_Wind_Resources$Wind)

summary(fullregression)
```
These results are not much better...try another type of variable reduction filter

##Varaible Reduction:Low Variance Filter
Sometimes variables are removed because they have low varaiance. Removing these variables will help to reduce the noise in the results of the PCA analysis. 
```{r Low Variance PCA}
#Removing variables with less than 20% of maximum varaince.
High_Variance_Variables <- subset(Variance_Sorted, Variance_Sorted[,1] >= ((0.2)*max(Variance_Sorted[,1])))
WVS_Data_Precentages5_Inverse <- t(WVS_Data_Precentages5)
High_Variance_Variables_Responses <- merge(High_Variance_Variables, WVS_Data_Precentages5_Inverse, by.x=0, by.y=0)
rownames(High_Variance_Variables_Responses) <- High_Variance_Variables_Responses$Row.names
High_Variance_Variables_Responses <- High_Variance_Variables_Responses[ ,c(3:58)]
High_Variance_Variables_Responses <- t(High_Variance_Variables_Responses)

#Now that we could to remove our reference category before running the PCA - however no questions retained all dummy variables after low variability dummy variables were removed therefore we did not remove the dummy variables.
WVS_Data_Precentages7<- High_Variance_Variables_Responses[, -grep("Ref", colnames(High_Variance_Variables_Responses))]

#This reduces are variables to 131 from the 617 that we started with 
High_Variance_Variables_Responses_Names <- as.data.frame(colnames(High_Variance_Variables_Responses))

PCA_3 <- prcomp(High_Variance_Variables_Responses)
summary(PCA_3)
plot(PCA_3, type = "l")
abline(h=3000, col="red")
```

```{r PCA3 Loadings}
#Loadings have variables as rows
PCA_Loadings3 <- round(PCA_3$rotation [,c(1:5)], 3)
head(PCA_Loadings3)
PCA_Loadings3 <- as.data.frame(PCA_Loadings3)
write.csv(PCA_Loadings3, file= "PCA_Loadings3.csv")

#Plots of Components
#pdf("Loading distribution3.pdf", paper = "USr")
par(mfrow=c(2,3))

plot(PCA_Loadings3$PC1, type = "l", main = "PC1")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings3$PC2, type = "l", main = "PC2")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings3$PC3, type = "l", main = "PC3")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings3$PC4, type = "l", main = "PC4")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(PCA_Loadings3$PC5, type = "l", main = "PC5")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

#dev.off()
```

```{r Cleaning out values smaller than 0.1 - PCA3}
PCA_Loadings3[abs(PCA_Loadings3) < 0.1] = NA
PCA_Loadings_Cleaned3 <- PCA_Loadings3[rowSums(is.na(PCA_Loadings3))!= 5, ]
write.csv(PCA_Loadings_Cleaned3, file= "PCA_Loadings_Cleaned3.csv")
head(PCA_Loadings_Cleaned3)

PCA_Scores3 <- PCA_3$x[,c(1:5)]
head(PCA_Scores3)

Named_PCA_Analysis_Data3 <- merge(Country_Names, PCA_Scores3, by.y= 0, by.x ="Country.Code", all.y=T)

Renewable_Scores3 <- merge(Renewable_merge, Named_PCA_Analysis_Data3,
                                   by.x="Country.Name", by.y="Country.Title")
Renewable_Scores3 <- Renewable_Scores3[,c(-4)]
Renewable_Scores3 <- na.omit(Renewable_Scores3)
```

###Varaible Reduction:Low Variance Filter - Final Data for Regression
This is the dataframe we will use to explore the relationships between renewable electricity production and cultural values. 
```{r}
Final_Data <- Renewable_Scores3
```

###Varaible Reduction:Low Variance Filter - Linear Regression
We will start by testing a linear regression with all 5 principle components and no control, moderator, or mediator variables
```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5)
summary(fullregression)
```
From these results we can see that only the first PC has a significant relationship with the precentage of renewables at a naitonal level. 

###Varaible Reduction:Low Variance Filter - Controlling for GDP
There are many variabales associated with the contruction on new infrastructure. The availability of funds to build the infrastructure being a major factor. Therefore, in our regression models we will control for GDP. One country does not have a 2015 GDP values, bringing us down to 52 countries.
```{r GDP Control Data3}
GDP_rawdata <- read.csv("World_Bank_GDP_Data.csv")
GDP_Years <- GDP_rawdata[4,]

#remove first 4 rows to bring column headers to top
GDP_data <- GDP_rawdata[c(5:286),]
colnames(GDP_data) <- GDP_Years

#remove all columns except for country names, and 2015 GDP
GDP_data2015 <-GDP_data[,c(1,60)]
colnames(GDP_data2015)<- c("Country.Title", "2015 GDP")

GDP_data_named<-merge(GDP_data2015, Country_Names, by.x= "Country.Title", by.y = "Country.Title")

GDP_Control <- merge(GDP_data_named, Final_Data, by.x = "Country.Title", by.y= "Country.Name")

Control <- GDP_Control$`2015 GDP`/10^9 #down to 53 countries
```

```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5 + Control)
summary(fullregression)
```
Nothing signifincatnly changes when we control for GDP, which is interesting because GDP is considered a moderating (Mediating?) variable in culture studies. 

###Varaible Reduction:Low Variance Filter - Other Controls
```{r}
Renewable_Resources <- read.csv("Renewable_Resources.csv")

Renewable_Resources<- merge(Renewable_Resources, Final_Data, by.x = "Country.Code", by.y = "Country.Code.x")

Renewable_Gen<- Renewable_Resources$RE_Generation.x
PC1 <- Renewable_Resources$PC1
PC2 <- Renewable_Resources$PC2
PC3 <- Renewable_Resources$PC3
PC4 <- Renewable_Resources$PC4
PC5 <- Renewable_Resources$PC5
Wind <- Renewable_Resources$Wind
Solar <- as.numeric(Renewable_Resources$Solar)
Policy <- Renewable_Resources$Policy.Target
```

```{r}
fullregression_resources <- lm(log(Renewable_Gen+1) ~ PC1 + PC2 + PC3 + PC4 + PC5 +
                                 Wind + Solar + Policy)
summary(fullregression_resources)
```

###Varaible Reduction:Low Variance Filter - Linear Regression with Wind Cost Overrun Data
```{r}
Wind_Cost_Overruns <- read.csv("Wind_Cost_Overrun_Data.csv")

Wind_Cost_Overruns_Final <- merge(Wind_Cost_Overruns, Country_Names, by.x = "Country", by.y = "Country.Title")
Cost_Performance_Wind <- merge(Wind_Cost_Overruns_Final, GDP_Control, by.x ="Country.Code", by.y = "Country.Code")
Cost_Performance_Wind_Resources <- merge(Cost_Performance_Wind, Renewable_Resources, 
                                         by.x ="Country.Code", by.y = "Country.Code")

fullregression <- lm(Cost_Performance_Wind$X.Cost.Over.Run.2017 ~ Cost_Performance_Wind$PC1 + 
                       Cost_Performance_Wind$PC2 + 
                       Cost_Performance_Wind$PC3 + 
                       Cost_Performance_Wind$PC4 + 
                       Cost_Performance_Wind$Offshore + Cost_Performance_Wind_Resources$Wind)

summary(fullregression)
```


##Varaible Reduction: Spearman Rank Correlations
When hofestede developed his cultural model and dimesnsions he used the spearman rank correlations to determine which variables to analyze further. For example for the construction of the power distance index, Hofstede choose one questions (measured on a likert scale), theorhetically and then looked for other varaibles (likert or porportions) which had strong (over 0.5), signifiant relationships with the orginal questions. He then retained the concepturally related varaibles which had the strongest most signifcant relationship to that orginial question. The final PDI index is based on three vairables.

```{r}
Spearman_Correlations <- cor(WVS_Data_Precentages5, method = "kendall")
Spearman_Correlations <- as.data.frame(Spearman_Correlations)
Spearman_Correlations[abs(Spearman_Correlations) < 0.7] = NA
#write.csv(Spearman_Correlations, "Spearman_Correlations_0.7Cleaned.csv")

#remove columns/rows which sum to 1 - because that implies that only correlate with themselves at 0.7 or higher
Spearman_Correlations_NoCorr<- Spearman_Correlations[,-(which(colSums(Spearman_Correlations, na.rm=TRUE) == 1.00))] 

#the list below is the variables to keep
Correlated_Variables <- names(Spearman_Correlations_NoCorr)

#There are 180 varaibles that do not correlate with anything else. They should be removed from the analysis. 
 Spearman_Correlations_0.7Corr<- Spearman_Correlations[Correlated_Variables, Correlated_Variables]
 diag(Spearman_Correlations_0.7Corr) =NA

#Now we want to only keep variables which have signifcant correlations beyond other variables in their question. We can do this by summing each row - excluding the variables related to the same question

#For example if we sum the first row excluding the variable from V4
sum(Spearman_Correlations_0.7Corr["V9_1_Ref",c(1:224)], na.rm=T)
#this means that V4_1_Ref is correlated to something beyond other V4 questions because the sum is not zero. In this example, V4_1_Ref have a -0.72 correlation with V143_3. We can use a significance test to determine if this is a statistically significant relationship. We need to use the raw individual data to do the signifcance test.
cor.test(WVS_Data_Precentages5$V9_1_Ref,WVS_Data_Precentages5$V49_1_Ref,method = "kendall")
#this test results that indicates that the relationship is significant and should be explored further. 

Spearman_Correlations_100<- Spearman_Correlations[c(1:100), c(1:100)]
Spearman_Correlations_25<- Spearman_Correlations[c(1:25), c(1:25)]
diag(Spearman_Correlations_25)=NA
corrplot(as.matrix(Spearman_Correlations_25))
```
From this plot it is still hard tell which correlations are outside the question. We will now remove all inter question correlations and test to see if the correlations that do result are statiscally significant
```{r}
#we want to keep all variables with correlations (outside it's own question) in the PCA analysis. Correlations between varaibles of the same question are an artifact of the research design. 

Eliminate_Kendall <- c("V4_","V4_","V8_","V8_","V9_","V9_","V9_",
                       "V19_","V25_","V25_","V25_","V26_","V26_","V26_",
                       "V27_","V27_","V27_","V28_","V28_","V29_",
                       "V29_","V30_","V30_","V30_","V31_","V31_",
                       "V31_","V32_","V32_","V32_","V33_","V33_",
                       "V33_","V34_","V34_","V34_","V37_","V40_", 
                       "V41_","V43_","V44_","V75_","V76_","V79_",
                       "V79_","V45_","V45_","V102_","V102_","V49_",
                       "V49_","V49_","V51_","V51_","V51_","V51_",
                       "V52_","V53_","V53_","V53_","V53_","V67_",
                       "V67_","V85_","V86_","V87_","V87_","V88_",
                       "V88_","V89_","V108_","V108_","V110_","V110_",
                       "V110_","V110_","V111_","V111_","V111_","V111_",
                       "V115_","V115_","V115_","V116_","V116_","V116_",
                       "V117_","V117_","V117_","V117_","V122_","V122_",
                       "V122_","V122_","V123_","V123_","V123_","V123_",
                       "V124_","V129_","V129_","V130_","V130_","V222_",
                       "V222_","V222_","V222_","V223_","V223_","V223_",
                       "V223_","V225_","V225_","V226_","V226_","V226_",
                       "V227_","V227_","V227_","V146_","V147_","V147_",
                       "V150_","V150_","V151_","V151_","V153_","V153_",
                       "V211_","V211_","V211_","V106_","V106_","V106_",
                       "V107_","V107_","V107_","V212_","V213_","V214_",
                       "V214_","V243_","V244_","V172_","V172_","V175_",
                       "V175_","V179_","V180_","V181_","V181_","V181_",
                       "V182_","V182_","V183_","V183_","V183_","V183_",
                       "V184_","V184_","V184_","V184_","V185_","V185_",
                       "V185_","V188_","V188_","V188_","V189_","V189_",
                       "V189_","V190_","V190_","V190_","V191_","V191_",
                       "V191_","V98_","V98_","V99_","V100_", "V132_",
                       "V132_","V198_","V198_","V198_","V200_","V200_",
                       "V200_","V209_","V209_","V209_","V199_","V199_",
                       "V199_","V201_","V201_","V202_","V203_","V203_",
                       "V203_","V203_","V204_","V204_","V204_","V204_",
                       "V205_","V205_","V205_","V207_","V207_","V207_",
                       "V207_","V206_","V208_","V208_","V208_","V208_",
                       "V231_","V232_","V232_","V233_","V163_","V163_")
  
  
  Eliminate_Spearman <- c("V4_", "V4_","V5_","V5_","V5_","V6_","V6_",
                        "V6_","V7_","V7_","V7_","V7_","V8_","V8_","V8_",
                        "V9_","V9_","V9_","V10_","V10_","V10_","V10_",
                        "V11_","V11_","V19_","V25_","V25_","V25_",
                        "V26_","V26_","V26_","V27_","V27_","V27_",
                        "V28_","V28_","V28_","V29_","V29_","V29_",
                        "V30_","V30_","V30_","V31_","V31_","V31_",
                        "V32_","V32_","V32_2","V33_","V33_","V33_","V34_",
                        "V34_","V34_","V35_","V35_","V35_","V37_","V42_",
                        "V39_","V38_", "V36_","V40_","V41_","V43_","V44_",
                        "V24_","V70_","V70_","V70_","V71_","V71_","V71_",
                        "V71_","V71_","V71_","V72_","V72_","V72_","V72_",
                        "V72_","V73_","V73_","V73_","V73_","V73_",
                        "V75_","V75_","V75_","V75_","V75_","V76_",
                        "V76_","V76_","V76_","V76_","V76_","V77_","V77_",
                        "V77_","V77_","V77_","V78_","V78_","V78_",
                        "V78_","V78_","V78_","V79_","V79_","V79_","V79_",
                        "V79_","V79_","V74_","V74_","V74_","V74_","V74_",
                        "V165_","V165_","V166_","V166_","V166_","V167_",
                        "V168_","V168_","V168_","V168_","V169_","V169_","V169_",
                        "V82_","V45_","V45_","V45_","V46_","V102_","V102_",
                        "V102_","V102_","V49_","V49_","V49_",
                        "V49_4","V54_","V54_","V54_","V54_","V51_","V51_",
                        "V51_","V51_","V52_","V52_","V52_","V52_",
                        "V50_1","V50_3", "V48_","V47_","V47_","V47_",
                        "V53_","V53_","V53_","V53_","V60_","V60_",
                        "V61_","V62_","V63_","V64_","V67_","V67_","V67_",
                        "V68_","V68_","V69_","V69_","V69_",
                        "V84_","V84_","V85_","V85_" ,"V85_","V86_",
                        "V86_","V86_","V87_","V87_","V87_","V88_",
                        "V88_","V88_","V89_","V89_","V89_",
                        "V108_","V108_","V108_","V109_","V109_","V109_",
                        "V109_","V110_","V110_","V110_","V110_","V111_",
                        "V111_","V111_","V111_","V112_", "V112_","V112_",
                        "V112_","V113_","V113_","V113_","V113_",
                        "V114_","V114_","V114_","V114_","V115_","V115_","V115_",
                        "V115_","V116_","V116_","V116_","V116_","V117_",
                        "V117_","V117_","V117_","V118_","V118_",
                        "V118_","V118_","V119_","V119_","V119_",
                        "V120_","V120_","V120_","V120_","V121_","V121_",
                        "V121_","V121_","V122_","V122_",
                        "V122_","V122_","V123_","V123_","V123_","V123_",
                        "V124_","V124_","V124_","V124_","V126_","V126_","V126_",
                        "V127_","V127_","V128_","V129_","V129_","V129_","V130_",
                        "V130_","V142_","V142_","V142_","V217_","V217_","V217_",
                        "V218_","V218_","V219_","V219_","V219_","V220_","V220_",
                        "V221_","V221_","V221_","V222_","V222_","V222_","V222_",
                        "V222_","V223_","V223_","V223_","V223_","V223_",
                        "V225_","V225_","V226_","V226_","V226_","V227_","V227_",
                        "V227_","V143_","V143_","V143_","V145_","V145_",
                        "V145_","V145_","V146_","V146_","V146_","V147_",
                        "V147_","V147_","V148_","V149_", "V150_","V150_","V150_",
                        "V151_","V151_","V151_","V153_","V153_","V153_","V153_",
                        "V154_","V154_","V154_","V155_","V155_","V156_",
                        "V156_","V156_","V211_","V211_","V211_","V211_",
                        "V103_","V103_","V103_","V104_","V104_","V104_",
                        "V106_","V106_","V106_","V106_","V107_","V107_",
                        "V107_","V107_","V212_","V212_","V212_","V212_",
                        "V213_","V213_","V213_","V213_","V214_",
                        "V214_","V214_","V214_","V216_","V216_","V216_",
                        "V243_","V244_","V170_","V170_","V171_","V171_",
                        "V171_","V171_","V172_","V172_","V172_",
                        "V173_","V173_","V173_","V173_","V174_","V174_",
                        "V174_","V174_", "V175_","V175_","V175_","V175_",
                        "V176_","V177_","V179_","V180_","V181_","V181_",
                        "V181_","V181_","V182_","V182_","V182_",
                        "V182_","V183_","V183_","V183_","V183_",
                        "V184_","V184_","V184_","V184_","V185_","V185_",
                        "V185_","V185_","V186_","V186_",
                        "V188_","V188_","V188_","V188_","V189_","V189_",
                        "V189_","V189_","V190_","V190_","V190_","V190_",
                        "V191_","V191_","V191_","V191_","V23_","V23_",
                        "V23_","V23_","V23_","V56_","V56_","V56_",
                        "V55_","V55_","V55_","V55_","V55_","V157_","V157_",
                        "V158_","V158_","V158","V159_","V159_","V159_",
                        "V159_","V160_","V164_","V164_","V59_",
                        "V59_","V59_","V59_ ","V96_","V96_","V96_",
                        "V96_","V97_","V97_","V97_","V98_","V98_","V98_",
                        "V98_","V99_","V99_","V99_","V100_",
                        "V100_","V100_","V100_","V101_",
                        "V101_","V101_","V101_","V131_","V131_","V131_",
                        "V131_","V132_","V132_","V132_","V132_","V132_",
                        "V134_","V134_","V134_","V134_",
                        "V134_","V135_","V135_","V135_","V135_","V135_",
                        "V136_","V136_","V136_","V136_",
                        "V136_","V137_","V137_","V137_","V137_","V138_",
                        "V138_","V138_","V141_","V141_",         
                        "V141_","V141_","V192_","V192_","V192_","V194_",
                        "V194_","V197_","V197_","V197_",         
                        "V197_","V197_","V198_","V198_","V198_","V198_",
                        "V198_","V200_","V200_","V200_",         
                        "V200_","V200_","V209_","V209_","V209_","V209_",
                        "V209_","V210_","V210_","V210_",
                        "V210_","V210_","V199_","V199_","V199_","V199_",
                        "V199_","V201_","V201_","V201_",
                        "V201_","V201_","V202_","V202_","V202_","V202_",
                        "V202_","V203_","V203_","V203_",
                        "V203_","V203_","V204_","V204_","V204_","V204_",
                        "V205_","V205_","V205_","V205_",
                        "V207_","V207_","V207_","V207_","V207_","V206_",
                        "V206_","V206_","V208_","V208_",
                        "V208_","V208_","V208_","V231_","V231_","V231_",
                        "V231_","V232_","V232_","V232_",
                        "V232_","V233_","V233_","V161_","V161_","V161_",
                        "V162_","V162_","V162_","V163_",
                        "V163_","V163_")
Keep <- c()
i=1
for (aRow in Correlated_Variables[c(1:224)]){
  DF <- abs(Spearman_Correlations_0.7Corr[ , -grep(Eliminate_Kendall[i], colnames(Spearman_Correlations_0.7Corr))])
  sum <- sum((DF[aRow,]), na.rm=T)
  if(sum>0){Keep <- cbind(Keep, aRow)} #if sum of row is greater than 0, keep the row.
  #if(is.na(sum==T)){next}
  i=i+1
}
Keep <- c(Keep[1,])

#There are (797-224=   ) varaibles that do not correlate with anything else above 0.7 or below -0.7 but we elimanted another (224-140___) for only being correclated to other variables within the same question. They should be removed from the analysis. 
WVS_Data_Precentages5_0.7Corr<- WVS_Data_Precentages5[,Keep]
Spearman_Correlations_0.7Corr_Reduced<- Spearman_Correlations[Keep, Keep]


#now we need ensure that all these correlations (outside the variable questions to one another) are significant
Result <- c()
Sig_Correlations <- c()

#while we know that there are 140 variables which have correlations greater than 0.7 with variables outside their own question, we recreate the orginal dataframes from the last loop in order to do a significance test. 

for(aRow in c(1:224)){
  DF <- Spearman_Correlations_0.7Corr[ , -grep(Eliminate_Kendall[aRow],colnames(Spearman_Correlations_0.7Corr))]
  Cell <- ncol(DF)
  i=1
#change this to referring to column/row name so that I make sure it's the right one. 
  for(i in c(1:Cell)){ #looping through each one of the columns in DF
    if (is.na(DF[aRow,i]) == FALSE) { #if the correlation exists, or the logical arguement is false, then run a correlation test)
      j <- colnames(DF[i])
      k <- rownames(DF[aRow,])
      #performing a correlation test between a row and a column - need to make sure it doesn't compare to itself
      Correlation_Test <- cor.test(WVS_Data_Precentages5[,k], WVS_Data_Precentages5[,j], method = "kendall")
      P_Values <- Correlation_Test$p.value
      R_squared <- DF[k,j]
      Result <- cbind(rownames(DF[k,]), colnames(DF[j]), R_squared, P_Values)
      Sig_Correlations <- rbind(Sig_Correlations, Result)
    }
    Result<- c()
  }
}

dim(Sig_Correlations)

#now we should delete any rows which have non-significant p-values
Sig_Correlations_Clean <- Sig_Correlations[!(as.numeric(Sig_Correlations[,4]) > 0.05),]
write.csv(Sig_Correlations_Clean, "Significant_Correlations.csv")

#looking at these correlations you can tell that there are variables that correlate strongly an entire questions and the correlation changes depending on the response of the variable. In order to reduce the amount of variables we could just take the postive relationships - selecting only one variable from each question that is positive and the strongest. 

#For example in the relationship between  

#we will choose the strongest postivie and negative relationships and then select the  single variable to represent the question based on the highest average correlation with the variables from the other questions. After we will ensure that we have not eliminated a variable which was uniquely correlated to one other variable.  

#Names of columns to keep in WVS_Data_Precentages5 to keep
VariablestoKeep <- unique(c(Sig_Correlations_Clean[,1],Sig_Correlations_Clean[,2]))
VariablestoKeep_DF <- as.data.frame(VariablestoKeep)
write.csv(VariablestoKeep_DF, "SpearmanCorrelation_Var.csv")

#Still has 500 variables....
WVS_Data_Precentages5_Spearman <- WVS_Data_Precentages5[,VariablestoKeep]
 
Spearman_PCA <- prcomp(WVS_Data_Precentages5_Spearman)
summary(Spearman_PCA)
plot(Spearman_PCA, type = "l")

Spearman.Varimax <-varimax(Spearman_PCA$rotation)
Spearman.Varimax.Loadings <- Spearman.Varimax.Loadings[,c(1:5)]


#try regressions with national data and then try to reduce the sample to older, educated, males - see if that helps clarify results.


test <- cor(WVS_Data_Precentages5[,"V143_3"], WVS_Data_Precentages5[,"V4_1_Ref"], method = "kendall")
```

#Varaible Reduction: Spearman Rank Correlations - PCA Loadings
```{r SpearmanPCA Loadings}
#Loadings have variables as rows
SpearmanPCA_Loadings <- round(Spearman_PCA$rotation [,c(1:5)], 3)
head(SpearmanPCA_Loadings)
SpearmanPCA_Loadings <- as.data.frame(SpearmanPCA_Loadings)
write.csv(SpearmanPCA_Loadings, file= "SpearmanPCA_Loadings.csv")

#Plots of Components
#pdf("Loading distribution3.pdf", paper = "USr")
par(mfrow=c(2,3))

plot(SpearmanPCA_Loadings$PC1, type = "l", main = "PC1")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(SpearmanPCA_Loadings$PC2, type = "l", main = "PC2")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(SpearmanPCA_Loadings$PC3, type = "l", main = "PC3")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(SpearmanPCA_Loadings$PC4, type = "l", main = "PC4")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

plot(SpearmanPCA_Loadings$PC5, type = "l", main = "PC5")
abline(h=0.05, col="red")
abline(h=-0.05, col="red")
abline(h=0.075, col="grey")
abline(h=-0.075, col="grey")
abline(h=0.1, col="blue")
abline(h=-0.1, col="blue")

#dev.off()
```
Unfortunately the loadings on these dimensions by different variables is still VERY low, no question loads greater than .3. Hofestede's dimensions had questions loading well above that as in 0.4 to 0.6 - this is because the questions were all focused on one topic - 10 questions related to workplace goals.  From this analysis we see that many are the same as the low variance filter. One way to increase the loading and interprett ability may be to choose only one variable for each question. To make this selection we would need to dig deeper into the non-parametric correlations and how the variables relate to others.

```{r}
corrplot(as.matrix(Spearman_Correlations_0.7Corr_Reduced[c(1:50), c(1:50) ]), 
         na.label = "square", na.label.col = "white", tl.cex = 0.5)
```
```{r}
#get rid of the correlations between variables of same questions

for(y in c(1:224)){
Spearman_Correlations_0.7Corr[grep(Eliminate_Kendall[y], colnames(Spearman_Correlations_0.7Corr)),
                              grep(Eliminate_Kendall[y], colnames(Spearman_Correlations_0.7Corr))] = NA
}

corrplot(as.matrix(Spearman_Correlations_0.7Corr[c(1:25), c(1:25) ]), 
         na.label = "square", na.label.col = "white", tl.cex = 0.5)

corrplot(as.matrix(Spearman_Correlations_0.7Corr[c(101:200), c(101:200) ]), 
         na.label = "square", na.label.col = "white", tl.cex = 0.5)


```


```{r Cleaning out values smaller than 0.1 - SpearmanPCA}
SpearmanPCA_Loadings[abs(SpearmanPCA_Loadings) < 0.1] = NA
SpearmanPCA_Loadings_Cleaned <- SpearmanPCA_Loadings[rowSums(is.na(SpearmanPCA_Loadings))!= 5, ]

write.csv(SpearmanPCA_Loadings_Cleaned, file= "SpearmanPCA_Loadings_Cleaned.csv")
head(SpearmanPCA_Loadings_Cleaned)

SpearmanPCA_Scores<- Spearman_PCA$x[,c(1:5)]
head(SpearmanPCA_Scores)

Named_SpearmanPCA_Scores <- merge(Country_Names, SpearmanPCA_Scores, by.y= 0, by.x ="Country.Code", all.y=T)

Renewable_Spearman_Scores <- merge(Renewable_merge, Named_SpearmanPCA_Scores,
                                   by.x="Country.Name", by.y="Country.Title")
Renewable_Spearman_Scores <- Renewable_Spearman_Scores[,c(1:7, 12:17)]
Renewable_Spearman_Scores<- na.omit(Renewable_Spearman_Scores)
```

###Varaible Reduction:Spearman Filter - Final Data for Regression
This is the dataframe we will use to explore the relationships between renewable electricity production and cultural values. 
```{r}
Final_Data <- Renewable_Spearman_Scores
```

###Varaible Reduction:Spearman Filter - Linear Regression
We will start by testing a linear regression with all 5 principle components and no control, moderator, or mediator variables
```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5)
summary(fullregression)
```


###Varaible Reduction:Low Variance Filter - Controlling for GDP
There are many variabales associated with the contruction on new infrastructure. The availability of funds to build the infrastructure being a major factor. Therefore, in our regression models we will control for GDP. One country does not have a 2015 GDP values, bringing us down to 52 countries.
```{r GDP Control Data3}
GDP_rawdata <- read.csv("World_Bank_GDP_Data.csv")
GDP_Years <- GDP_rawdata[4,]

#remove first 4 rows to bring column headers to top
GDP_data <- GDP_rawdata[c(5:286),]
colnames(GDP_data) <- GDP_Years

#remove all columns except for country names, and 2015 GDP
GDP_data2015 <-GDP_data[,c(1,60)]
colnames(GDP_data2015)<- c("Country.Title", "2015 GDP")

GDP_data_named<-merge(GDP_data2015, Country_Names, by.x= "Country.Title", by.y = "Country.Title")

GDP_Control <- merge(GDP_data_named, Final_Data, by.x = "Country.Title", by.y= "Country.Name")

Control <- GDP_Control$`2015 GDP`/10^9 #down to 53 countries
```

```{r}
fullregression <- lm(log(Final_Data$RE_Generation +1) ~ Final_Data$PC1 + Final_Data$PC2 + 
                      Final_Data$PC3 + Final_Data$PC4 + Final_Data$PC5 + Control)
summary(fullregression)
```

###Varaible Reduction:Kendall Corrleations - Other Controls
```{r}
Renewable_Resources <- read.csv("Renewable_Resources.csv")

Renewable_Resources<- merge(Renewable_Resources, Final_Data, by.x = "Country.Code", by.y = "Country.Code")

Renewable_Gen<- Renewable_Resources$RE_Generation.x
PC1 <- Renewable_Resources$PC1
PC2 <- Renewable_Resources$PC2
PC3 <- Renewable_Resources$PC3
PC4 <- Renewable_Resources$PC4
PC5 <- Renewable_Resources$PC5
Wind <- Renewable_Resources$Wind
Solar <- as.numeric(Renewable_Resources$Solar)
Policy <- Renewable_Resources$Policy.Target
```

```{r}
fullregression_resources <- lm(log(Renewable_Gen+1) ~ PC1 + PC2 + PC3 + PC4 + PC5 +
                                 Wind + Solar + Policy)
summary(fullregression_resources)
```

###Varaible Reduction:Low Variance Filter - Linear Regression with Wind Cost Overrun Data
```{r}
Wind_Cost_Overruns <- read.csv("Wind_Cost_Overrun_Data.csv")

Wind_Cost_Overruns_Final <- merge(Wind_Cost_Overruns, Country_Names, by.x = "Country", by.y = "Country.Title")
Cost_Performance_Wind <- merge(Wind_Cost_Overruns_Final, GDP_Control, by.x ="Country.Code", by.y = "Country.Code.x")
Cost_Performance_Wind_Resources <- merge(Cost_Performance_Wind, Renewable_Resources, 
                                         by.x ="Country.Code", by.y = "Country.Code")

fullregression <- lm(Cost_Performance_Wind$X.Cost.Over.Run.2017 ~ Cost_Performance_Wind$PC1 + 
                       Cost_Performance_Wind$PC2 + 
                       Cost_Performance_Wind$PC3 + 
                       Cost_Performance_Wind$PC4 + 
                       Cost_Performance_Wind$Offshore + Cost_Performance_Wind_Resources$Wind)

summary(fullregression)
```







#Non Linear Regression
```{r eval=FALSE}
y = Final_Data$RE_Generation
PC1 = Final_Data$PC1
PC2 = Final_Data$PC2
PC3 = Final_Data$PC3
PC4 = Final_Data$PC4

fit_1 <- nls(y ~ a^PC1 + b^PC2)
#letter c is a function of some type....

cor(y, predict(fit_1))
summary(fit_1)
```















#Analysis at an Invidiual Level
Theorhetically we should be able to perform reduce the individual level metrics into the same dimensions found in the aggregated data. To do this, we must have a data frame which includes dummy variables for all the categories of the different questions. Every variable will be coded as 0 or 1 depending on if the individual choose that category or not. 

We have removed the same questions and countries due to missingness and have removed the dummy variables associated with -1, -2, and -6.

We do not want to do a latent class analysis because we want to compare nations on a numerical scale. A factor analysis on Binary Data allows us to accomplish this. 
```{r}
#install.packages("FactoMineR")
library("FactoMineR")

#install.packages("ltm")
library("ltm")

#install.packages("lessR")
#install.packages("devtools")
library("lessR")
library("devtools")

#install_github('philchalmers/mirt')

#install.packages("stats4")
library("stats4")
#install.packages("lattice")
library("lattice")

library('mirt')

#install.packages("psych")
library("psych")
#install.packages("stats")
library("stats")

PAF_Test <- factor.pa(WVS_Data_Precentages6, 5)
FA_Test <- fa(WVS_Data_Precentages6,5)
```

## Analysis at an Invidiual Level: Cleaning
```{r}
Individual_Data<- merge(Updated_Categorical_data,Updated_Ordinal_data, by.x = "X", by.y = "X")

#Questions to Remove based on national sensitivity analysis.

#Due to high precentages of AdminNA remove questions: V74B,V90,V91,V92,V93,V94,v160A,160B,v160C
#v160D,v160E,v160F,v160G,v160H,v160I,v160J,V217_ESMA,V218_ESMA,V224_ESMA,V220_ESMA,V221_ESMA,V222_ESMA,V228A,
#V228B,V228C,V228D,V228E,V228F,V228G,V228H,V228I,V228J,V228K,V243_AU,V244_AU,V56_NZ,V203A,V207A

#Due to high precentages of Missing remove questions - none

#Due to high precentages of Don'tKnow remove questions: V133,V140,V193,V152

#Questions 144, 215 (political organizations), 241, 242 (age), 247 (language), 249 (age of complete school),254 (ethnicity), 256 were removed beause they were coded using political organizations, religion, age, languages, regions and ethnicities 
#219_ESMA was removed because it was not answered by any of the 60 countries.
#228 and 228_2 were removed because they are related to political parties in specific countries
#Removed all other sociodemographic questions "V57"  "V58"  "V229" "V230" "V234" "V235" "V236" "V237" "V238" "V240" "V248" "V250" "V253" "V254" "V255" "V256"

#Make one DF with all the single question dataframes

Cleaned_Individual_Data<- Individual_Data[,-which(names(Individual_Data) %in% c(
"V144", "V247", "V254","V256","V219_ESMA", "V228", "V228_2", "V57","V58","V229","V230","V234","V235",
"V236","V237","V238","V240","V248","V250", "V253","V255","V256", "V74B","V90","V91","V92","V93","V94",
"v160A","v160B","v160C","v160D","v160E", "v160F","v160G","v160H","v160I","v160J",
"V217_ESMA","V218_ESMA","V224_ESMA","V220_ESMA","V221_ESMA","V222_ESMA","V228A","V228B", "V228C","V228D","V228E",
"V228F","V228G","V228H","V228I","V228J", "V228K","V243_AU","V244_AU","V56_NZ","V203A","V207A","V133","V140","V193",
"V152", "V215", "V241", 
"V242", "V249", "V215_01","V215_02","V215_03","V215_04","V215_05","V215_06","V215_07",
"V215_08","V215_09","V215_10","V215_11","V215_12","V215_13","V215_14","V215_15","V215_16","V215_17","V215_18"))]

#We need to remove the countries chosen above
countries_to_remove_ind <-  c(414, 634,48,818)
Cleaned_Individual_Data <- Cleaned_Individual_Data[-which(Cleaned_Individual_Data$V2.x %in% c(414,634,48,818)),]


RecodetoNA<- function(QuestionColName){
  QuestionColName<- recode(QuestionColName, "c(-1,-2,-6) = NA")
}

Cleaned_Individual_Data_NA <- as.data.frame(apply(Cleaned_Individual_Data,2,RecodetoNA))
Cleaned_Individual_Data_NA <- Cleaned_Individual_Data_NA[,2:227]
  
write.csv(Cleaned_Individual_Data, "Cleaned_Individual_Data_Raw.csv")

Individual_Data_Final <- Cleaned_Individual_Data[,4:227]
```

##Analysis at an Invidiual Level: ANOVA
In order to understand which variables are contributing to the variance of the questions repsonses, we ran ANOVA tests on the randomly selected variables.  Understanding that there are demographic variables which are affectin the responses will help to more accurately aggregate the data at a national level. For those variables which significantly affect the variance of the responses, the question variable will be subdivided by the affecting variable. 

First we must make different dataframe at the individual level. This dataframe is different from the one used in the MIRT model because we have added the country code, gender, education, and age back into the dataframe. Country, Gender, and Education were recorded as categorical variables; however age had to be grouped by decades to create a factor or categorical variables. 
```{r}
#Removing the Questions from the sensitivity analysis same as above
Cleaned_Individual_Data_ANOVA<- Individual_Data[,-which(names(Individual_Data) %in% c(
"V144", "V247", "V254","V256","V219_ESMA", "V228", "V228_2", 
"V57","V58","V229","V230","V234","V235","V236","V237","V238",
"V250", "V253","V255","V256", "V74B","V90","V91","V92","V93",
"V94","v160A","v160B","v160C","v160D","v160E",  "v160F",
"v160G","v160H","v160I","v160J",
"V217_ESMA","V218_ESMA","V224_ESMA","V220_ESMA","V221_ESMA","V222_ESMA","V228A","V228B", "V228C","V228D","V228E",
"V228F","V228G","V228H","V228I","V228J", "V228K","V243_AU","V244_AU","V56_NZ","V203A","V207A","V133","V140","V193",
"V152", "V215", "V241", 
"V249", "V215_01","V215_02","V215_03","V215_04","V215_05","V215_06","V215_07",
"V215_08","V215_09","V215_10","V215_11","V215_12","V215_13","V215_14","V215_15","V215_16","V215_17","V215_18"))]

#Adding Age variable that has been categorized
Cleaned_Individual_Data_ANOVA <- cbind(Cleaned_Individual_Data_ANOVA, SD_FreeResponse$Age_Cat, SD_FreeResponse$V242)

Cleaned_Individual_Data_ANOVA <- Cleaned_Individual_Data_ANOVA[-which(Cleaned_Individual_Data_ANOVA$V2.x %in% c(414,634,48,818)),]


RecodetoNA<- function(QuestionColName){
  QuestionColName<- recode(QuestionColName, "c(-1,-2,-6) = NA")
}

Cleaned_Individual_Data_NA_ANOVA <- as.data.frame(apply(Cleaned_Individual_Data_ANOVA,2,RecodetoNA))
```

The ANOVA analysis is done on question individually. Below is code to randomly select a quesiton for each iteration. The code prints the selected question title
```{r}
column_number <- sample(1:224, 1)
colname <- colnames(Individual_Data_Final)[column_number]
colname
question <- as.data.frame(cbind(Cleaned_Individual_Data_NA_ANOVA[,colname], Cleaned_Individual_Data_NA_ANOVA$V2.x,
                  Cleaned_Individual_Data_NA_ANOVA$V240, Cleaned_Individual_Data_NA_ANOVA$V248,
                  Cleaned_Individual_Data_NA_ANOVA$`SD_FreeResponse$Age_Cat`,
                  Cleaned_Individual_Data_NA_ANOVA$`SD_FreeResponse$V242`))
colnames(question)<- c("Question_Response", "Country.Code", "Gender", "Education", "Age_Cat", "Age")
```

A primary premise of this analysis is that the country significantly affects  - with the exception of country since that is what we intend to study.
```{r}
Anova_Model_Country <- lm(question$Question_Response ~ as.factor(question$Country.Code) 
                  + question$Gender + question$Age_Cat)
anova(Anova_Model_Country)
```

```{r}
Anova_Model_Education <- lm(question$Question_Response ~ as.factor(question$Education) 
                  + question$Gender + question$Age_Cat)
anova(Anova_Model_Education)
```
```{r}
Anova_Model<- lm(question$Question_Response ~ as.factor(question$Education) + as.factor(question$Country.Code)
                  + question$Gender + question$Age_Cat)
anova(Anova_Model)
```

##Analysis at an Invidiual Level: Discriminant Analysis
Unfortunatley this is using the questions responses as numerical answers which they are NOT - therefore this anlaysis should not be run. 
```{r}
Lda_test <- question
lda <- lda(as.factor(Lda_test$Country.Code) ~ Lda_test$Question_Response)
summary(lda)
```

##Analysis at an Invidiual Level: ANOVA Results
10 random questions results
```{r}
#To combine multiple questions into a single dataframe 

i = 0
questions = c()
Question_Anova_Model_Summary_Fvalues <- c()
Question_Anova_Model_Summary_Pvalues <- c()

for (i in 1:10){
  column_number <- sample(1:224, 1)
  colname <- colnames(Individual_Data_Final)[column_number]
  colname
  questions <- c(questions,colname)
  question <- as.data.frame(cbind(Cleaned_Individual_Data_NA_ANOVA[,colname], 
                                  Cleaned_Individual_Data_NA_ANOVA$V2.x,
                                  Cleaned_Individual_Data_NA_ANOVA$V240, 
                                  Cleaned_Individual_Data_NA_ANOVA$V248,
                                  Cleaned_Individual_Data_NA_ANOVA$`SD_FreeResponse$Age_Cat`))
  colnames(question)<- c("Question_Response", "Country.Code", "Gender", "Education", "Age_Cat")
  Anova_Model<- lm(question$Question_Response ~ as.factor(question$Education) + 
                     as.factor(question$Country.Code)
                  + question$Gender + question$Age_Cat)
  
  Anova_Model_Summary <- as.data.frame(anova(Anova_Model))
  
  Question_Anova_Model_Summary_Fvalues <- cbind(Question_Anova_Model_Summary_Fvalues, 
                                        Anova_Model_Summary$`F value`)
  
  Question_Anova_Model_Summary_Pvalues <- cbind(Question_Anova_Model_Summary_Pvalues, 
                                         Anova_Model_Summary$`Pr(>F)`)
  i = i+1
}

Question_Anova_Model_Summary_Fvalues <- t(as.data.frame(Question_Anova_Model_Summary_Fvalues))
rownames(Question_Anova_Model_Summary_Fvalues) <- questions
colnames(Question_Anova_Model_Summary_Fvalues) <-c("Education", "Country", "Gender", "Age_Cat", "Residuals")

Question_Anova_Model_Summary_Pvalues <- t(as.data.frame(Question_Anova_Model_Summary_Pvalues))
rownames(Question_Anova_Model_Summary_Pvalues) <- questions
colnames(Question_Anova_Model_Summary_Pvalues) <-c("Education", "Country", "Gender", "Age_Cat", "Residuals")
Question_Anova_Model_Summary_Pvalues
```

This analysis shows us that country, education, gender, and age all have significant effects on the variance of the questions responses. When hofstede found similar results, he proceeded to divide his responses by occupation, in our case education and then average the subgroups to get the country score he used in his dimension calculatuion. 

To replicate those steps, we would need to divide by each country into 9 subgroups divided by their education level, determine the % of people who answered in each those categories. We could then average the precentages of all the subgroups; however, since we have a nationally representative sample, where countries provided each individual a weight. Subdiving and averaging - ignoring the weights will not provide an accurate relationships


##Analysis at an Invidiual Level: Variable Reduction at Indvidual Level - NOT COMPLETE
```{r eval=FALSE}
library(dplyr)
#install.packages("purrr")
library(purrr)
#install.packages("tidyr")
library(tidyr)
#install.packages("ggplot2")
library(ggplot2)

#Need to analyze data at the indvidual level for each question with dummy variables. 
Questions <- colnames(Cleaned_Individual_Data_NA[,4:226])

# select and prepare question data
Cleaned_Individual_Data_Binary <- Cleaned_Individual_Data_NA %>%
  #select(one_of(c("V2.x","V258.x"), Questions)) %>% # select country, weight, and questions
  tibble::rownames_to_column(var = "id")# make row numbers explicit subject ids
 

# The following produces a data set with observations of individual responses and
# columns for each level of each question with values in {0, 1} indicating response.

# `spread(data, X, X, sep = "_")` used as below will take a single column X to a set of columns,
# one for each level of X, with names given by "X_level" and entries given by `level` if the original entry was
# `level` and `NA` otherwise

d <- map(Questions, ~ spread_(select_(Cleaned_Individual_Data_Binary, "id", .), 
                           ., ., sep = "_")) %>% # map `spread` over question columns
        reduce(inner_join) %>% # join all resulting data frames by "id"
        mutate_at(vars(everything(), -id), funs(ifelse(is.na(.), 0, 1))) %>% # recode entries to 0s and 1s
        {inner_join(select(Cleaned_Individual_Data_Binary, 
                           id, country, weight), ., by = "id")} # join with country codes and weights

#Then looking at the significance values, we will remove the questions that do not have significant p-values. 
V4_LDA <- na.omit(V4)
V4_LDA<- V4_LDA[,-(which(colSums(V4_LDA) == 0))] 
V4_LDA <- add_rownames(V4_LDA, "Country")
library("MASS")
lda(Country ~ ., data = V4_Count_LDA)

```

#MIRT
```{r eval=FALSE}
Indiv_Model <- mirt(Individual_Data_Final, 4, itemtype = "nominal", survey.weights = Cleaned_Individual_Data$V258.x)
Indiv_Model_Summary <- summary(Indiv_Model)

Oblimin_Factors <- Indiv_Model_Summary$rotF
write.csv(Oblimin_Factors, "Indiv_Factor_Loadings.csv")
plot(Oblimin_Factors[,1])
```


